"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.logWrapper = exports.callMethodSync = exports.callMethodAsync = void 0;
const n8n_workflow_1 = require("n8n-workflow");
const tools_1 = require("@langchain/core/tools");
const chat_history_1 = require("@langchain/core/chat_history");
const embeddings_1 = require("@langchain/core/embeddings");
const vectorstores_1 = require("@langchain/core/vectorstores");
const textsplitters_1 = require("@langchain/textsplitters");
const chat_memory_1 = require("@langchain/community/memory/chat_memory");
const retrievers_1 = require("@langchain/core/retrievers");
const output_parsers_1 = require("@langchain/core/output_parsers");
const lodash_1 = require("lodash");
const N8nJsonLoader_1 = require("./N8nJsonLoader");
const N8nBinaryLoader_1 = require("./N8nBinaryLoader");
const helpers_1 = require("./helpers");
const errorsMap = {
    'You exceeded your current quota, please check your plan and billing details.': {
        message: 'OpenAI quota exceeded',
        description: 'You exceeded your current quota, please check your plan and billing details.',
    },
};
async function callMethodAsync(parameters) {
    try {
        return await parameters.method.call(this, ...parameters.arguments);
    }
    catch (e) {
        if (e instanceof output_parsers_1.OutputParserException)
            throw e;
        if (e.functionality === 'configuration-node')
            throw e;
        const connectedNode = parameters.executeFunctions.getNode();
        const error = new n8n_workflow_1.NodeOperationError(connectedNode, e, {
            functionality: 'configuration-node',
        });
        if (errorsMap[error.message]) {
            error.description = errorsMap[error.message].description;
            error.message = errorsMap[error.message].message;
        }
        parameters.executeFunctions.addOutputData(parameters.connectionType, parameters.currentNodeRunIndex, error);
        if (error.message) {
            error.description = error.message;
            throw error;
        }
        throw new n8n_workflow_1.NodeOperationError(connectedNode, `Error on node "${connectedNode.name}" which is connected via input "${parameters.connectionType}"`, { functionality: 'configuration-node' });
    }
}
exports.callMethodAsync = callMethodAsync;
function callMethodSync(parameters) {
    try {
        return parameters.method.call(this, ...parameters.arguments);
    }
    catch (e) {
        if (e.functionality === 'configuration-node')
            throw e;
        const connectedNode = parameters.executeFunctions.getNode();
        const error = new n8n_workflow_1.NodeOperationError(connectedNode, e);
        parameters.executeFunctions.addOutputData(parameters.connectionType, parameters.currentNodeRunIndex, error);
        throw new n8n_workflow_1.NodeOperationError(connectedNode, `Error on node "${connectedNode.name}" which is connected via input "${parameters.connectionType}"`, { functionality: 'configuration-node' });
    }
}
exports.callMethodSync = callMethodSync;
function logWrapper(originalInstance, executeFunctions) {
    return new Proxy(originalInstance, {
        get: (target, prop) => {
            let connectionType;
            if (originalInstance instanceof chat_memory_1.BaseChatMemory) {
                if (prop === 'loadMemoryVariables' && 'loadMemoryVariables' in target) {
                    return async (values) => {
                        var _a;
                        connectionType = "ai_memory";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { action: 'loadMemoryVariables', values } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [values],
                        }));
                        const chatHistory = (_a = response === null || response === void 0 ? void 0 : response.chat_history) !== null && _a !== void 0 ? _a : response;
                        executeFunctions.addOutputData(connectionType, index, [
                            [{ json: { action: 'loadMemoryVariables', chatHistory } }],
                        ]);
                        return response;
                    };
                }
                else if (prop === 'saveContext' && 'saveContext' in target) {
                    return async (input, output) => {
                        connectionType = "ai_memory";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { action: 'saveContext', input, output } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [input, output],
                        }));
                        const chatHistory = await target.chatHistory.getMessages();
                        executeFunctions.addOutputData(connectionType, index, [
                            [{ json: { action: 'saveContext', chatHistory } }],
                        ]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof chat_history_1.BaseChatMessageHistory) {
                if (prop === 'getMessages' && 'getMessages' in target) {
                    return async () => {
                        connectionType = "ai_memory";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { action: 'getMessages' } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [],
                        }));
                        const payload = { action: 'getMessages', response };
                        executeFunctions.addOutputData(connectionType, index, [[{ json: payload }]]);
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.memory.get.messages', { response });
                        return response;
                    };
                }
                else if (prop === 'addMessage' && 'addMessage' in target) {
                    return async (message) => {
                        connectionType = "ai_memory";
                        const payload = { action: 'addMessage', message };
                        const { index } = executeFunctions.addInputData(connectionType, [[{ json: payload }]]);
                        await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [message],
                        });
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.memory.added.message', { message });
                        executeFunctions.addOutputData(connectionType, index, [[{ json: payload }]]);
                    };
                }
            }
            if (originalInstance instanceof output_parsers_1.BaseOutputParser) {
                if (prop === 'parse' && 'parse' in target) {
                    return async (text) => {
                        var _a, _b;
                        connectionType = "ai_outputParser";
                        const stringifiedText = (0, lodash_1.isObject)(text) ? JSON.stringify(text) : text;
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { action: 'parse', text: stringifiedText } }],
                        ]);
                        try {
                            const response = (await callMethodAsync.call(target, {
                                executeFunctions,
                                connectionType,
                                currentNodeRunIndex: index,
                                method: target[prop],
                                arguments: [stringifiedText],
                            }));
                            void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.output.parser.parsed', { text, response });
                            executeFunctions.addOutputData(connectionType, index, [
                                [{ json: { action: 'parse', response } }],
                            ]);
                            return response;
                        }
                        catch (error) {
                            void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.output.parser.parsed', {
                                text,
                                response: (_a = error.message) !== null && _a !== void 0 ? _a : error,
                            });
                            executeFunctions.addOutputData(connectionType, index, [
                                [{ json: { action: 'parse', response: (_b = error.message) !== null && _b !== void 0 ? _b : error } }],
                            ]);
                            throw error;
                        }
                    };
                }
            }
            if (originalInstance instanceof retrievers_1.BaseRetriever) {
                if (prop === 'getRelevantDocuments' && 'getRelevantDocuments' in target) {
                    return async (query, config) => {
                        connectionType = "ai_retriever";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { query, config } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [query, config],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.retriever.get.relevant.documents', { query });
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof embeddings_1.Embeddings) {
                if (prop === 'embedDocuments' && 'embedDocuments' in target) {
                    return async (documents) => {
                        connectionType = "ai_embedding";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { documents } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [documents],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.embeddings.embedded.document');
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
                if (prop === 'embedQuery' && 'embedQuery' in target) {
                    return async (query) => {
                        connectionType = "ai_embedding";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { query } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [query],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.embeddings.embedded.query');
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof N8nJsonLoader_1.N8nJsonLoader ||
                originalInstance instanceof N8nBinaryLoader_1.N8nBinaryLoader) {
                if (prop === 'processAll' && 'processAll' in target) {
                    return async (items) => {
                        connectionType = "ai_document";
                        const { index } = executeFunctions.addInputData(connectionType, [items]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [items],
                        }));
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
                if (prop === 'processItem' && 'processItem' in target) {
                    return async (item, itemIndex) => {
                        connectionType = "ai_document";
                        const { index } = executeFunctions.addInputData(connectionType, [[item]]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [item, itemIndex],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.document.processed');
                        executeFunctions.addOutputData(connectionType, index, [
                            [{ json: { response }, pairedItem: { item: itemIndex } }],
                        ]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof textsplitters_1.TextSplitter) {
                if (prop === 'splitText' && 'splitText' in target) {
                    return async (text) => {
                        connectionType = "ai_textSplitter";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { textSplitter: text } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [text],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.text.splitter.split');
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof tools_1.Tool) {
                if (prop === '_call' && '_call' in target) {
                    return async (query) => {
                        connectionType = "ai_tool";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { query } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [query],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.tool.called', { query, response });
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
            }
            if (originalInstance instanceof vectorstores_1.VectorStore) {
                if (prop === 'similaritySearch' && 'similaritySearch' in target) {
                    return async (query, k, filter, _callbacks) => {
                        connectionType = "ai_vectorStore";
                        const { index } = executeFunctions.addInputData(connectionType, [
                            [{ json: { query, k, filter } }],
                        ]);
                        const response = (await callMethodAsync.call(target, {
                            executeFunctions,
                            connectionType,
                            currentNodeRunIndex: index,
                            method: target[prop],
                            arguments: [query, k, filter, _callbacks],
                        }));
                        void (0, helpers_1.logAiEvent)(executeFunctions, 'n8n.ai.vector.store.searched', { query });
                        executeFunctions.addOutputData(connectionType, index, [[{ json: { response } }]]);
                        return response;
                    };
                }
            }
            return target[prop];
        },
    });
}
exports.logWrapper = logWrapper;
//# sourceMappingURL=logWrapper.js.map