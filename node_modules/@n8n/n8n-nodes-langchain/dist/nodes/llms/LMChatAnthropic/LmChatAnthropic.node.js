"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LmChatAnthropic = void 0;
const anthropic_1 = require("@langchain/anthropic");
const sharedFields_1 = require("../../../utils/sharedFields");
const N8nLlmTracing_1 = require("../N8nLlmTracing");
const modelField = {
    displayName: 'Model',
    name: 'model',
    type: 'options',
    options: [
        {
            name: 'Claude 3 Opus(20240229)',
            value: 'claude-3-opus-20240229',
        },
        {
            name: 'Claude 3 Sonnet(20240229)',
            value: 'claude-3-sonnet-20240229',
        },
        {
            name: 'Claude 3 Haiku(20240307)',
            value: 'claude-3-haiku-20240307',
        },
        {
            name: 'LEGACY: Claude 2',
            value: 'claude-2',
        },
        {
            name: 'LEGACY: Claude 2.1',
            value: 'claude-2.1',
        },
        {
            name: 'LEGACY: Claude Instant 1.2',
            value: 'claude-instant-1.2',
        },
        {
            name: 'LEGACY: Claude Instant 1',
            value: 'claude-instant-1',
        },
    ],
    description: 'The model which will generate the completion. <a href="https://docs.anthropic.com/claude/docs/models-overview">Learn more</a>.',
    default: 'claude-2',
};
class LmChatAnthropic {
    constructor() {
        this.description = {
            displayName: 'Anthropic Chat Model',
            name: 'lmChatAnthropic',
            icon: 'file:anthropic.svg',
            group: ['transform'],
            version: [1, 1.1],
            description: 'Language Model Anthropic',
            defaults: {
                name: 'Anthropic Chat Model',
            },
            codex: {
                categories: ['AI'],
                subcategories: {
                    AI: ['Language Models'],
                },
                resources: {
                    primaryDocumentation: [
                        {
                            url: 'https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/',
                        },
                    ],
                },
                alias: ['claude', 'sonnet', 'opus'],
            },
            inputs: [],
            outputs: ["ai_languageModel"],
            outputNames: ['Model'],
            credentials: [
                {
                    name: 'anthropicApi',
                    required: true,
                },
            ],
            properties: [
                (0, sharedFields_1.getConnectionHintNoticeField)(["ai_chain", "ai_chain"]),
                {
                    ...modelField,
                    displayOptions: {
                        show: {
                            '@version': [1],
                        },
                    },
                },
                {
                    ...modelField,
                    default: 'claude-3-sonnet-20240229',
                    displayOptions: {
                        hide: {
                            '@version': [1],
                        },
                    },
                },
                {
                    displayName: 'Options',
                    name: 'options',
                    placeholder: 'Add Option',
                    description: 'Additional options to add',
                    type: 'collection',
                    default: {},
                    options: [
                        {
                            displayName: 'Maximum Number of Tokens',
                            name: 'maxTokensToSample',
                            default: 4096,
                            description: 'The maximum number of tokens to generate in the completion',
                            type: 'number',
                        },
                        {
                            displayName: 'Sampling Temperature',
                            name: 'temperature',
                            default: 0.7,
                            typeOptions: { maxValue: 1, minValue: 0, numberPrecision: 1 },
                            description: 'Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.',
                            type: 'number',
                        },
                        {
                            displayName: 'Top K',
                            name: 'topK',
                            default: -1,
                            typeOptions: { maxValue: 1, minValue: -1, numberPrecision: 1 },
                            description: 'Used to remove "long tail" low probability responses. Defaults to -1, which disables it.',
                            type: 'number',
                        },
                        {
                            displayName: 'Top P',
                            name: 'topP',
                            default: 1,
                            typeOptions: { maxValue: 1, minValue: 0, numberPrecision: 1 },
                            description: 'Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.',
                            type: 'number',
                        },
                    ],
                },
            ],
        };
    }
    async supplyData(itemIndex) {
        const credentials = await this.getCredentials('anthropicApi');
        const modelName = this.getNodeParameter('model', itemIndex);
        const options = this.getNodeParameter('options', itemIndex, {});
        const tokensUsageParser = (llmOutput) => {
            var _a;
            const usage = (_a = llmOutput === null || llmOutput === void 0 ? void 0 : llmOutput.usage) !== null && _a !== void 0 ? _a : {
                input_tokens: 0,
                output_tokens: 0,
            };
            return {
                completionTokens: usage.output_tokens,
                promptTokens: usage.input_tokens,
                totalTokens: usage.input_tokens + usage.output_tokens,
            };
        };
        const model = new anthropic_1.ChatAnthropic({
            anthropicApiKey: credentials.apiKey,
            modelName,
            maxTokens: options.maxTokensToSample,
            temperature: options.temperature,
            topK: options.topK,
            topP: options.topP,
            callbacks: [new N8nLlmTracing_1.N8nLlmTracing(this, { tokensUsageParser })],
        });
        return {
            response: model,
        };
    }
}
exports.LmChatAnthropic = LmChatAnthropic;
//# sourceMappingURL=LmChatAnthropic.node.js.map