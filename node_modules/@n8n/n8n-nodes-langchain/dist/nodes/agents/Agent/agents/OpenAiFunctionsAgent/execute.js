"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.openAiFunctionsAgentExecute = void 0;
const n8n_workflow_1 = require("n8n-workflow");
const agents_1 = require("langchain/agents");
const prompts_1 = require("@langchain/core/prompts");
const output_parsers_1 = require("langchain/output_parsers");
const memory_1 = require("langchain/memory");
const openai_1 = require("@langchain/openai");
const helpers_1 = require("../../../../../utils/helpers");
const tracing_1 = require("../../../../../utils/tracing");
async function openAiFunctionsAgentExecute(nodeVersion) {
    var _a;
    this.logger.verbose('Executing OpenAi Functions Agent');
    const model = (await this.getInputConnectionData("ai_languageModel", 0));
    if (!(model instanceof openai_1.ChatOpenAI)) {
        throw new n8n_workflow_1.NodeOperationError(this.getNode(), 'OpenAI Functions Agent requires OpenAI Chat Model');
    }
    const memory = (await this.getInputConnectionData("ai_memory", 0));
    const tools = await (0, helpers_1.getConnectedTools)(this, nodeVersion >= 1.5);
    const outputParsers = await (0, helpers_1.getOptionalOutputParsers)(this);
    const options = this.getNodeParameter('options', 0, {});
    const agentConfig = {
        tags: ['openai-functions'],
        agent: agents_1.OpenAIAgent.fromLLMAndTools(model, tools, {
            prefix: options.systemMessage,
        }),
        tools,
        maxIterations: (_a = options.maxIterations) !== null && _a !== void 0 ? _a : 10,
        returnIntermediateSteps: (options === null || options === void 0 ? void 0 : options.returnIntermediateSteps) === true,
        memory: memory !== null && memory !== void 0 ? memory : new memory_1.BufferMemory({
            returnMessages: true,
            memoryKey: 'chat_history',
            inputKey: 'input',
            outputKey: 'output',
        }),
    };
    const agentExecutor = agents_1.AgentExecutor.fromAgentAndTools(agentConfig);
    const returnData = [];
    let outputParser;
    let prompt;
    if (outputParsers.length) {
        outputParser =
            outputParsers.length === 1 ? outputParsers[0] : new output_parsers_1.CombiningOutputParser(...outputParsers);
        const formatInstructions = outputParser.getFormatInstructions();
        prompt = new prompts_1.PromptTemplate({
            template: '{input}\n{formatInstructions}',
            inputVariables: ['input'],
            partialVariables: { formatInstructions },
        });
    }
    const items = this.getInputData();
    for (let itemIndex = 0; itemIndex < items.length; itemIndex++) {
        try {
            let input;
            if (this.getNode().typeVersion <= 1.2) {
                input = this.getNodeParameter('text', itemIndex);
            }
            else {
                input = (0, helpers_1.getPromptInputByType)({
                    ctx: this,
                    i: itemIndex,
                    inputKey: 'text',
                    promptTypeKey: 'promptType',
                });
            }
            if (input === undefined) {
                throw new n8n_workflow_1.NodeOperationError(this.getNode(), 'The ‘text‘ parameter is empty.');
            }
            if (prompt) {
                input = (await prompt.invoke({ input })).value;
            }
            let response = await agentExecutor
                .withConfig((0, tracing_1.getTracingConfig)(this))
                .invoke({ input, outputParsers });
            if (outputParser) {
                response = { output: await outputParser.parse(response.output) };
            }
            returnData.push({ json: response });
        }
        catch (error) {
            if (this.continueOnFail(error)) {
                returnData.push({ json: { error: error.message }, pairedItem: { item: itemIndex } });
                continue;
            }
            throw error;
        }
    }
    return [returnData];
}
exports.openAiFunctionsAgentExecute = openAiFunctionsAgentExecute;
//# sourceMappingURL=execute.js.map