[
{"displayName":"OpenAI","name":"@n8n/n8n-nodes-langchain.openAi","group":["transform"],"version":[1,1.1,1.2,1.3,1.4],"subtitle":"={{((resource, operation) => {\n    if (operation === 'deleteAssistant') {\n        return 'Delete Assistant';\n    }\n    if (operation === 'deleteFile') {\n        return 'Delete File';\n    }\n    if (operation === 'classify') {\n        return 'Classify Text';\n    }\n    if (operation === 'message' && resource === 'text') {\n        return 'Message Model';\n    }\n    const capitalize = (str) => {\n        const chars = str.split('');\n        chars[0] = chars[0].toUpperCase();\n        return chars.join('');\n    };\n    if (['transcribe', 'translate'].includes(operation)) {\n        resource = 'recording';\n    }\n    if (operation === 'list') {\n        resource = resource + 's';\n    }\n    return `${capitalize(operation)} ${capitalize(resource)}`;\n})($parameter.resource, $parameter.operation)}}","description":"Message an assistant or GPT, analyze images, generate audio, etc.","defaults":{"name":"OpenAI"},"codex":{"alias":["LangChain","ChatGPT","DallE","whisper","audio","transcribe","tts","assistant"],"categories":["AI"],"subcategories":{"AI":["Agents","Miscellaneous","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/app-nodes/n8n-nodes-langchain.openai/"}]}},"inputs":"={{((resource, operation, hideTools) => {\n    if (resource === 'assistant' && operation === 'message') {\n        return [\n            { type: \"main\" },\n            { type: \"ai_memory\", displayName: 'Memory', maxConnections: 1 },\n            { type: \"ai_tool\", displayName: 'Tools' },\n        ];\n    }\n    if (resource === 'text' && operation === 'message') {\n        if (hideTools === 'hide') {\n            return [\"main\"];\n        }\n        return [\n            { type: \"main\" },\n            { type: \"ai_tool\", displayName: 'Tools' },\n        ];\n    }\n    return [\"main\"];\n})($parameter.resource, $parameter.operation, $parameter.hideTools)}}","outputs":["main"],"credentials":[{"name":"openAiApi","required":true}],"properties":[{"displayName":"Resource","name":"resource","type":"options","noDataExpression":true,"options":[{"name":"Assistant","value":"assistant"},{"name":"Text","value":"text"},{"name":"Image","value":"image"},{"name":"Audio","value":"audio"},{"name":"File","value":"file"}],"default":"text"},{"displayName":"Operation","name":"operation","type":"options","noDataExpression":true,"options":[{"name":"Create an Assistant","value":"create","action":"Create an assistant","description":"Create a new assistant"},{"name":"Delete an Assistant","value":"deleteAssistant","action":"Delete an assistant","description":"Delete an assistant from the account"},{"name":"List Assistants","value":"list","action":"List assistants","description":"List assistants in the organization"},{"name":"Message an Assistant","value":"message","action":"Message an assistant","description":"Send messages to an assistant"},{"name":"Update an Assistant","value":"update","action":"Update an assistant","description":"Update an existing assistant"}],"default":"message","displayOptions":{"show":{"resource":["assistant"]}}},{"displayName":"Model","name":"modelId","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"modelSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. gpt-4"}],"displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Name","name":"name","type":"string","default":"","description":"The name of the assistant. The maximum length is 256 characters.","placeholder":"e.g. My Assistant","required":true,"displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Description","name":"description","type":"string","default":"","description":"The description of the assistant. The maximum length is 512 characters.","placeholder":"e.g. My personal assistant","displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Instructions","name":"instructions","type":"string","description":"The system instructions that the assistant uses. The maximum length is 32768 characters.","default":"","typeOptions":{"rows":2},"displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Code Interpreter","name":"codeInterpreter","type":"boolean","default":false,"description":"Whether to enable the code interpreter that allows the assistants to write and run Python code in a sandboxed execution environment, find more <a href=\"https://platform.openai.com/docs/assistants/tools/code-interpreter\" target=\"_blank\">here</a>","displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Knowledge Retrieval","name":"knowledgeRetrieval","type":"boolean","default":false,"description":"Whether to augments the assistant with knowledge from outside its model, such as proprietary product information or documents, find more <a href=\"https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\" target=\"_blank\">here</a>","displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Files","name":"file_ids","type":"multiOptions","description":"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant. You can use expression to pass file IDs as an array or comma-separated string.","typeOptions":{"loadOptionsMethod":"getFiles"},"default":[],"hint":"Add more files by using the 'Upload a File' operation","displayOptions":{"show":{"codeInterpreter":[true],"operation":["create"],"resource":["assistant"]},"hide":{"knowledgeRetrieval":[true]}}},{"displayName":"Files","name":"file_ids","type":"multiOptions","description":"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant","typeOptions":{"loadOptionsMethod":"getFiles"},"default":[],"hint":"Add more files by using the 'Upload a File' operation","displayOptions":{"show":{"knowledgeRetrieval":[true],"operation":["create"],"resource":["assistant"]},"hide":{"codeInterpreter":[true]}}},{"displayName":"Files","name":"file_ids","type":"multiOptions","description":"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant","typeOptions":{"loadOptionsMethod":"getFiles"},"default":[],"hint":"Add more files by using the 'Upload a File' operation","displayOptions":{"show":{"knowledgeRetrieval":[true],"codeInterpreter":[true],"operation":["create"],"resource":["assistant"]}}},{"displayName":"Add custom n8n tools when you <i>message</i> your assistant (rather than when creating it)","name":"noticeTools","type":"notice","default":"","displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Output Randomness (Temperature)","name":"temperature","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Output Randomness (Top P)","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Fail if Assistant Already Exists","name":"failIfExists","type":"boolean","default":false,"description":"Whether to fail an operation if the assistant with the same name already exists"}],"displayOptions":{"show":{"operation":["create"],"resource":["assistant"]}}},{"displayName":"Assistant","name":"assistantId","type":"resourceLocator","description":"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\"https://platform.openai.com/playground?mode=assistant\" target=\"_blank\">playground</a>.","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"assistantSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. asst_abc123"}],"displayOptions":{"show":{"operation":["deleteAssistant"],"resource":["assistant"]}}},{"displayName":"Assistant","name":"assistantId","type":"resourceLocator","description":"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\"https://platform.openai.com/playground?mode=assistant\" target=\"_blank\">playground</a>.","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"assistantSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. asst_abc123"}],"displayOptions":{"show":{"operation":["message"],"resource":["assistant"]}}},{"displayName":"Prompt","name":"prompt","type":"options","options":[{"name":"Take from previous node automatically","value":"auto","description":"Looks for an input field called chatInput"},{"name":"Define below","value":"define","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"auto","displayOptions":{"show":{"operation":["message"],"resource":["assistant"]}}},{"displayName":"Text","name":"text","type":"string","default":"","placeholder":"e.g. Hello, how can you help me?","typeOptions":{"rows":2},"displayOptions":{"show":{"prompt":["define"],"operation":["message"],"resource":["assistant"]}}},{"displayName":"Connect your own custom n8n tools to this node on the canvas","name":"noticeTools","type":"notice","default":"","displayOptions":{"show":{"operation":["message"],"resource":["assistant"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Timeout","name":"timeout","default":10000,"description":"Maximum amount of time a request is allowed to take in milliseconds","type":"number"},{"displayName":"Preserve Original Tools","name":"preserveOriginalTools","type":"boolean","default":true,"description":"Whether to preserve the original tools of the assistant after the execution of this node, otherwise the tools will be replaced with the connected tools, if any, default is true","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.3}}]}}}],"displayOptions":{"show":{"operation":["message"],"resource":["assistant"]}}},{"displayName":"Simplify Output","name":"simplify","type":"boolean","default":true,"description":"Whether to return a simplified version of the response instead of the raw data","displayOptions":{"show":{"operation":["list"],"resource":["assistant"]}}},{"displayName":"Assistant","name":"assistantId","type":"resourceLocator","description":"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\"https://platform.openai.com/playground?mode=assistant\" target=\"_blank\">playground</a>.","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"assistantSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. asst_abc123"}],"displayOptions":{"show":{"operation":["update"],"resource":["assistant"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Code Interpreter","name":"codeInterpreter","type":"boolean","default":false,"description":"Whether to enable the code interpreter that allows the assistants to write and run Python code in a sandboxed execution environment, find more <a href=\"https://platform.openai.com/docs/assistants/tools/code-interpreter\" target=\"_blank\">here</a>"},{"displayName":"Description","name":"description","type":"string","default":"","description":"The description of the assistant. The maximum length is 512 characters.","placeholder":"e.g. My personal assistant"},{"displayName":"Files","name":"file_ids","type":"multiOptions","description":"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant. You can use expression to pass file IDs as an array or comma-separated string.","typeOptions":{"loadOptionsMethod":"getFiles"},"default":[],"hint":"Add more files by using the 'Upload a File' operation, any existing files not selected here will be removed."},{"displayName":"Instructions","name":"instructions","type":"string","description":"The system instructions that the assistant uses. The maximum length is 32768 characters.","default":"","typeOptions":{"rows":2}},{"displayName":"Knowledge Retrieval","name":"knowledgeRetrieval","type":"boolean","default":false,"description":"Whether to augments the assistant with knowledge from outside its model, such as proprietary product information or documents, find more <a href=\"https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\" target=\"_blank\">here</a>"},{"displayName":"Model","name":"modelId","type":"resourceLocator","default":{"mode":"list","value":""},"required":false,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"modelSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. gpt-4"}]},{"displayName":"Name","name":"name","type":"string","default":"","description":"The name of the assistant. The maximum length is 256 characters.","placeholder":"e.g. My Assistant"},{"displayName":"Remove All Custom Tools (Functions)","name":"removeCustomTools","type":"boolean","default":false,"description":"Whether to remove all custom tools (functions) from the assistant"},{"displayName":"Output Randomness (Temperature)","name":"temperature","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Output Randomness (Top P)","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}],"displayOptions":{"show":{"operation":["update"],"resource":["assistant"]}}},{"displayName":"Operation","name":"operation","type":"options","noDataExpression":true,"options":[{"name":"Generate Audio","value":"generate","action":"Generate audio","description":"Creates audio from a text prompt"},{"name":"Transcribe a Recording","value":"transcribe","action":"Transcribe a recording","description":"Transcribes audio into the text"},{"name":"Translate a Recording","value":"translate","action":"Translate a recording","description":"Translate audio into the text in the english language"}],"default":"generate","displayOptions":{"show":{"resource":["audio"]}}},{"displayName":"OpenAI API limits the size of the audio file to 25 MB","name":"fileSizeLimitNotice","type":"notice","default":" ","displayOptions":{"show":{"resource":["audio"],"operation":["translate","transcribe"]}}},{"displayName":"Model","name":"model","type":"options","default":"tts-1","options":[{"name":"TTS-1","value":"tts-1"},{"name":"TTS-1-HD","value":"tts-1-hd"}],"displayOptions":{"show":{"operation":["generate"],"resource":["audio"]}}},{"displayName":"Text Input","name":"input","type":"string","placeholder":"e.g. The quick brown fox jumped over the lazy dog","description":"The text to generate audio for. The maximum length is 4096 characters.","default":"","typeOptions":{"rows":2},"displayOptions":{"show":{"operation":["generate"],"resource":["audio"]}}},{"displayName":"Voice","name":"voice","type":"options","default":"alloy","description":"The voice to use when generating the audio","options":[{"name":"Alloy","value":"alloy"},{"name":"Echo","value":"echo"},{"name":"Fable","value":"fable"},{"name":"Nova","value":"nova"},{"name":"Onyx","value":"onyx"},{"name":"Shimmer","value":"shimmer"}],"displayOptions":{"show":{"operation":["generate"],"resource":["audio"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Response Format","name":"response_format","type":"options","default":"mp3","options":[{"name":"MP3","value":"mp3"},{"name":"OPUS","value":"opus"},{"name":"AAC","value":"aac"},{"name":"FLAC","value":"flac"}]},{"displayName":"Audio Speed","name":"speed","type":"number","default":1,"typeOptions":{"minValue":0.25,"maxValue":4,"numberPrecision":1}},{"displayName":"Put Output in Field","name":"binaryPropertyOutput","type":"string","default":"data","hint":"The name of the output field to put the binary file data in"}],"displayOptions":{"show":{"operation":["generate"],"resource":["audio"]}}},{"displayName":"Input Data Field Name","name":"binaryPropertyName","type":"string","default":"data","placeholder":"e.g. data","hint":"The name of the input field containing the binary file data to be processed","description":"Name of the binary property which contains the audio file in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm","displayOptions":{"show":{"operation":["transcribe"],"resource":["audio"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Language of the Audio File","name":"language","type":"string","description":"The language of the input audio. Supplying the input language in <a href=\"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes\" target=\"_blank\">ISO-639-1</a> format will improve accuracy and latency.","default":""},{"displayName":"Output Randomness (Temperature)","name":"temperature","type":"number","default":0,"typeOptions":{"minValue":0,"maxValue":1,"numberPrecision":1}}],"displayOptions":{"show":{"operation":["transcribe"],"resource":["audio"]}}},{"displayName":"Input Data Field Name","name":"binaryPropertyName","type":"string","default":"data","hint":"The name of the input field containing the binary file data to be processed","placeholder":"e.g. data","description":"Name of the binary property which contains the audio file in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm","displayOptions":{"show":{"operation":["translate"],"resource":["audio"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Output Randomness (Temperature)","name":"temperature","type":"number","default":0,"typeOptions":{"minValue":0,"maxValue":1,"numberPrecision":1}}],"displayOptions":{"show":{"operation":["translate"],"resource":["audio"]}}},{"displayName":"Operation","name":"operation","type":"options","noDataExpression":true,"options":[{"name":"Delete a File","value":"deleteFile","action":"Delete a file","description":"Delete a file from the server"},{"name":"List Files","value":"list","action":"List files","description":"Returns a list of files that belong to the user's organization"},{"name":"Upload a File","value":"upload","action":"Upload a file","description":"Upload a file that can be used across various endpoints"}],"default":"upload","displayOptions":{"show":{"resource":["file"]}}},{"displayName":"Input Data Field Name","name":"binaryPropertyName","type":"string","default":"data","hint":"The name of the input field containing the binary file data to be processed","placeholder":"e.g. data","description":"Name of the binary property which contains the file. The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants.","displayOptions":{"show":{"operation":["upload"],"resource":["file"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Purpose","name":"purpose","type":"options","default":"assistants","description":"The intended purpose of the uploaded file, the 'Fine-tuning' only supports .jsonl files","options":[{"name":"Assistants","value":"assistants"},{"name":"Fine-Tune","value":"fine-tune"}]}],"displayOptions":{"show":{"operation":["upload"],"resource":["file"]}}},{"displayName":"File","name":"fileId","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"fileSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","validation":[{"type":"regex","properties":{"regex":"file-[a-zA-Z0-9]","errorMessage":"Not a valid File ID"}}],"placeholder":"e.g. file-1234567890"}],"displayOptions":{"show":{"operation":["deleteFile"],"resource":["file"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Purpose","name":"purpose","type":"options","default":"any","description":"Only return files with the given purpose","options":[{"name":"Any [Default]","value":"any"},{"name":"Assistants","value":"assistants"},{"name":"Fine-Tune","value":"fine-tune"}]}],"displayOptions":{"show":{"operation":["list"],"resource":["file"]}}},{"displayName":"Operation","name":"operation","type":"options","noDataExpression":true,"options":[{"name":"Analyze Image","value":"analyze","action":"Analyze image","description":"Take in images and answer questions about them"},{"name":"Generate an Image","value":"generate","action":"Generate an image","description":"Creates an image from a text prompt"}],"default":"generate","displayOptions":{"show":{"resource":["image"]}}},{"displayName":"Model","name":"model","type":"options","default":"dall-e-3","description":"The model to use for image generation","options":[{"name":"DALL-E-2","value":"dall-e-2"},{"name":"DALL-E-3","value":"dall-e-3"}],"displayOptions":{"show":{"operation":["generate"],"resource":["image"]}}},{"displayName":"Prompt","name":"prompt","type":"string","placeholder":"e.g. A cute cat eating a dinosaur","description":"A text description of the desired image(s). The maximum length is 1000 characters for dall-e-2 and 4000 characters for dall-e-3.","default":"","typeOptions":{"rows":2},"displayOptions":{"show":{"operation":["generate"],"resource":["image"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Number of Images","name":"n","default":1,"description":"Number of images to generate","type":"number","typeOptions":{"minValue":1,"maxValue":10},"displayOptions":{"show":{"/model":["dall-e-2"]}}},{"displayName":"Quality","name":"quality","type":"options","description":"The quality of the image that will be generated, HD creates images with finer details and greater consistency across the image","options":[{"name":"HD","value":"hd"},{"name":"Standard","value":"standard"}],"displayOptions":{"show":{"/model":["dall-e-3"]}},"default":"standard"},{"displayName":"Resolution","name":"size","type":"options","options":[{"name":"256x256","value":"256x256"},{"name":"512x512","value":"512x512"},{"name":"1024x1024","value":"1024x1024"}],"displayOptions":{"show":{"/model":["dall-e-2"]}},"default":"1024x1024"},{"displayName":"Resolution","name":"size","type":"options","options":[{"name":"1024x1024","value":"1024x1024"},{"name":"1792x1024","value":"1792x1024"},{"name":"1024x1792","value":"1024x1792"}],"displayOptions":{"show":{"/model":["dall-e-3"]}},"default":"1024x1024"},{"displayName":"Style","name":"style","type":"options","options":[{"name":"Natural","value":"natural","description":"Produce more natural looking images"},{"name":"Vivid","value":"vivid","description":"Lean towards generating hyper-real and dramatic images"}],"displayOptions":{"show":{"/model":["dall-e-3"]}},"default":"vivid"},{"displayName":"Respond with Image URL(s)","name":"returnImageUrls","type":"boolean","default":false,"description":"Whether to return image URL(s) instead of binary file(s)"},{"displayName":"Put Output in Field","name":"binaryPropertyOutput","type":"string","default":"data","hint":"The name of the output field to put the binary file data in","displayOptions":{"show":{"returnImageUrls":[false]}}}],"displayOptions":{"show":{"operation":["generate"],"resource":["image"]}}},{"displayName":"Model","name":"modelId","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"imageModelSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. gpt-4"}],"displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.4}}],"operation":["analyze"],"resource":["image"]}}},{"displayName":"Text Input","name":"text","type":"string","placeholder":"e.g. What's in this image?","default":"What's in this image?","typeOptions":{"rows":2},"displayOptions":{"show":{"operation":["analyze"],"resource":["image"]}}},{"displayName":"Input Type","name":"inputType","type":"options","default":"url","options":[{"name":"Image URL(s)","value":"url"},{"name":"Binary File(s)","value":"base64"}],"displayOptions":{"show":{"operation":["analyze"],"resource":["image"]}}},{"displayName":"URL(s)","name":"imageUrls","type":"string","placeholder":"e.g. https://example.com/image.jpeg","description":"URL(s) of the image(s) to analyze, multiple URLs can be added separated by comma","default":"","displayOptions":{"show":{"inputType":["url"],"operation":["analyze"],"resource":["image"]}}},{"displayName":"Input Data Field Name","name":"binaryPropertyName","type":"string","default":"data","placeholder":"e.g. data","hint":"The name of the input field containing the binary file data to be processed","description":"Name of the binary property which contains the image(s)","displayOptions":{"show":{"inputType":["base64"],"operation":["analyze"],"resource":["image"]}}},{"displayName":"Simplify Output","name":"simplify","type":"boolean","default":true,"description":"Whether to simplify the response or not","displayOptions":{"show":{"operation":["analyze"],"resource":["image"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Detail","name":"detail","type":"options","default":"auto","options":[{"name":"Auto","value":"auto","description":"Model will look at the image input size and decide if it should use the low or high setting"},{"name":"Low","value":"low","description":"Return faster responses and consume fewer tokens"},{"name":"High","value":"high","description":"Return more detailed responses, consumes more tokens"}]},{"displayName":"Length of Description (Max Tokens)","description":"Fewer tokens will result in shorter, less detailed image description","name":"maxTokens","type":"number","default":300,"typeOptions":{"minValue":1}}],"displayOptions":{"show":{"operation":["analyze"],"resource":["image"]}}},{"displayName":"Operation","name":"operation","type":"options","noDataExpression":true,"options":[{"name":"Message a Model","value":"message","action":"Message a model","description":"Create a completion with GPT 3, 4, etc."},{"name":"Classify Text for Violations","value":"classify","action":"Classify text for violations","description":"Check whether content complies with usage policies"}],"default":"message","displayOptions":{"show":{"resource":["text"]}}},{"displayName":"Text Input","name":"input","type":"string","placeholder":"e.g. Sample text goes here","description":"The input text to classify if it is violates the moderation policy","default":"","typeOptions":{"rows":2},"displayOptions":{"show":{"operation":["classify"],"resource":["text"]}}},{"displayName":"Simplify Output","name":"simplify","type":"boolean","default":false,"description":"Whether to return a simplified version of the response instead of the raw data","displayOptions":{"show":{"operation":["classify"],"resource":["text"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Use Stable Model","name":"useStableModel","type":"boolean","default":false,"description":"Whether to use the stable version of the model instead of the latest version, accuracy may be slightly lower"}],"displayOptions":{"show":{"operation":["classify"],"resource":["text"]}}},{"displayName":"Model","name":"modelId","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"modelSearch","searchable":true}},{"displayName":"ID","name":"id","type":"string","placeholder":"e.g. gpt-4"}],"displayOptions":{"show":{"operation":["message"],"resource":["text"]}}},{"displayName":"Messages","name":"messages","type":"fixedCollection","typeOptions":{"sortable":true,"multipleValues":true},"placeholder":"Add Message","default":{"values":[{"content":""}]},"options":[{"displayName":"Values","name":"values","values":[{"displayName":"Text","name":"content","type":"string","description":"The content of the message to be send","default":"","typeOptions":{"rows":2}},{"displayName":"Role","name":"role","type":"options","description":"Role in shaping the model's response, it tells the model how it should behave and interact with the user","options":[{"name":"User","value":"user","description":"Send a message as a user and get a response from the model"},{"name":"Assistant","value":"assistant","description":"Tell the model to adopt a specific tone or personality"},{"name":"System","value":"system","description":"Usually used to set the model's behavior or context for the next user message"}],"default":"user"}]}],"displayOptions":{"show":{"operation":["message"],"resource":["text"]}}},{"displayName":"Simplify Output","name":"simplify","type":"boolean","default":true,"description":"Whether to return a simplified version of the response instead of the raw data","displayOptions":{"show":{"operation":["message"],"resource":["text"]}}},{"displayName":"Output Content as JSON","name":"jsonOutput","type":"boolean","description":"Whether to attempt to return the response in JSON format. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.","default":false,"displayOptions":{"show":{"operation":["message"],"resource":["text"]}}},{"displayName":"Hide Tools","name":"hideTools","type":"hidden","default":"hide","displayOptions":{"show":{"modelId":["gpt-3.5-turbo-16k-0613","dall-e-3","text-embedding-3-large","dall-e-2","whisper-1","tts-1-hd-1106","tts-1-hd","gpt-4-0314","text-embedding-3-small","gpt-4-32k-0314","gpt-3.5-turbo-0301","gpt-4-vision-preview","gpt-3.5-turbo-16k","gpt-3.5-turbo-instruct-0914","tts-1","davinci-002","gpt-3.5-turbo-instruct","babbage-002","tts-1-1106","text-embedding-ada-002"],"@version":[{"_cnd":{"gte":1.2}}],"operation":["message"],"resource":["text"]}}},{"displayName":"Connect your own custom n8n tools to this node on the canvas","name":"noticeTools","type":"notice","default":"","displayOptions":{"hide":{"hideTools":["hide"]},"show":{"operation":["message"],"resource":["text"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Frequency Penalty","name":"frequency_penalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":16,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Number of Completions","name":"n","default":1,"description":"How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.","type":"number"},{"displayName":"Presence Penalty","name":"presence_penalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Output Randomness (Temperature)","name":"temperature","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Output Randomness (Top P)","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}],"displayOptions":{"show":{"operation":["message"],"resource":["text"]}}}],"iconUrl":{"light":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vendors/OpenAi/openAi.svg","dark":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vendors/OpenAi/openAi.dark.svg"}},
{"displayName":"AI Agent","name":"@n8n/n8n-nodes-langchain.agent","icon":"fa:robot","iconColor":"black","group":["transform"],"version":[1,1.1,1.2,1.3,1.4,1.5,1.6],"description":"Generates an action plan and executes it. Can use external tools.","subtitle":"={{ {\ttoolsAgent: 'Tools Agent', conversationalAgent: 'Conversational Agent', openAiFunctionsAgent: 'OpenAI Functions Agent', reActAgent: 'ReAct Agent', sqlAgent: 'SQL Agent', planAndExecuteAgent: 'Plan and Execute Agent' }[$parameter.agent] }}","defaults":{"name":"AI Agent","color":"#404040"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Agents","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/"}]}},"inputs":"={{\n\t\t\t((agent, hasOutputParser) => {\n\t\t\t\tfunction getInputs(agent, hasOutputParser) {\n    const getInputData = (inputs) => {\n        const displayNames = {\n            [\"ai_languageModel\"]: 'Model',\n            [\"ai_memory\"]: 'Memory',\n            [\"ai_tool\"]: 'Tool',\n            [\"ai_outputParser\"]: 'Output Parser',\n        };\n        return inputs.map(({ type, filter }) => {\n            const input = {\n                type,\n                displayName: type in displayNames ? displayNames[type] : undefined,\n                required: type === \"ai_languageModel\",\n                maxConnections: [\"ai_languageModel\", \"ai_memory\"].includes(type)\n                    ? 1\n                    : undefined,\n            };\n            if (filter) {\n                input.filter = filter;\n            }\n            return input;\n        });\n    };\n    let specialInputs = [];\n    if (agent === 'conversationalAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n                filter: {\n                    nodes: [\n                        '@n8n/n8n-nodes-langchain.lmChatAnthropic',\n                        '@n8n/n8n-nodes-langchain.lmChatGroq',\n                        '@n8n/n8n-nodes-langchain.lmChatOllama',\n                        '@n8n/n8n-nodes-langchain.lmChatOpenAi',\n                        '@n8n/n8n-nodes-langchain.lmChatGooglePalm',\n                        '@n8n/n8n-nodes-langchain.lmChatGoogleGemini',\n                        '@n8n/n8n-nodes-langchain.lmChatMistralCloud',\n                        '@n8n/n8n-nodes-langchain.lmChatAzureOpenAi',\n                    ],\n                },\n            },\n            {\n                type: \"ai_memory\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'toolsAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n                filter: {\n                    nodes: [\n                        '@n8n/n8n-nodes-langchain.lmChatAnthropic',\n                        '@n8n/n8n-nodes-langchain.lmChatAzureOpenAi',\n                        '@n8n/n8n-nodes-langchain.lmChatMistralCloud',\n                        '@n8n/n8n-nodes-langchain.lmChatOpenAi',\n                        '@n8n/n8n-nodes-langchain.lmChatGroq',\n                    ],\n                },\n            },\n            {\n                type: \"ai_memory\",\n            },\n            {\n                type: \"ai_tool\",\n                required: true,\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'openAiFunctionsAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n                filter: {\n                    nodes: [\n                        '@n8n/n8n-nodes-langchain.lmChatOpenAi',\n                        '@n8n/n8n-nodes-langchain.lmChatAzureOpenAi',\n                    ],\n                },\n            },\n            {\n                type: \"ai_memory\",\n            },\n            {\n                type: \"ai_tool\",\n                required: true,\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'reActAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    else if (agent === 'sqlAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n            },\n            {\n                type: \"ai_memory\",\n            },\n        ];\n    }\n    else if (agent === 'planAndExecuteAgent') {\n        specialInputs = [\n            {\n                type: \"ai_languageModel\",\n            },\n            {\n                type: \"ai_tool\",\n            },\n            {\n                type: \"ai_outputParser\",\n            },\n        ];\n    }\n    if (hasOutputParser === false) {\n        specialInputs = specialInputs.filter((input) => input.type !== \"ai_outputParser\");\n    }\n    return [\"main\", ...getInputData(specialInputs)];\n};\n\t\t\t\treturn getInputs(agent, hasOutputParser)\n\t\t\t})($parameter.agent, $parameter.hasOutputParser === undefined || $parameter.hasOutputParser === true)\n\t\t}}","outputs":["main"],"credentials":[{"name":"mySql","required":true,"testedBy":"mysqlConnectionTest","displayOptions":{"show":{"agent":["sqlAgent"],"/dataSource":["mysql"]}}},{"name":"postgres","required":true,"displayOptions":{"show":{"agent":["sqlAgent"],"/dataSource":["postgres"]}}}],"properties":[{"displayName":"Save time with an <a href=\"/templates/1954\" target=\"_blank\">example</a> of how this node works","name":"notice","type":"notice","default":"","displayOptions":{"show":{"agent":["conversationalAgent"]}}},{"displayName":"Agent","name":"agent","type":"options","noDataExpression":true,"options":[{"name":"Conversational Agent","value":"conversationalAgent","description":"Selects tools to accomplish its task and uses memory to recall previous conversations"},{"name":"OpenAI Functions Agent","value":"openAiFunctionsAgent","description":"Utilizes OpenAI's Function Calling feature to select the appropriate tool and arguments for execution"},{"name":"Plan and Execute Agent","value":"planAndExecuteAgent","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks"},{"name":"ReAct Agent","value":"reActAgent","description":"Strategically select tools to accomplish a given task"},{"name":"SQL Agent","value":"sqlAgent","description":"Answers questions about data in an SQL database"}],"default":"conversationalAgent","displayOptions":{"show":{"@version":[{"_cnd":{"lte":1.5}}]}}},{"displayName":"Agent","name":"agent","type":"options","noDataExpression":true,"options":[{"name":"Tools Agent","value":"toolsAgent","description":"Utilized unified Tool calling interface to select the appropriate tools and argument for execution"},{"name":"Conversational Agent","value":"conversationalAgent","description":"Selects tools to accomplish its task and uses memory to recall previous conversations"},{"name":"OpenAI Functions Agent","value":"openAiFunctionsAgent","description":"Utilizes OpenAI's Function Calling feature to select the appropriate tool and arguments for execution"},{"name":"Plan and Execute Agent","value":"planAndExecuteAgent","description":"Plan and execute agents accomplish an objective by first planning what to do, then executing the sub tasks"},{"name":"ReAct Agent","value":"reActAgent","description":"Strategically select tools to accomplish a given task"},{"name":"SQL Agent","value":"sqlAgent","description":"Answers questions about data in an SQL database"}],"default":"toolsAgent","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.6}}]}}},{"displayName":"Prompt","name":"promptType","type":"options","options":[{"name":"Take from previous node automatically","value":"auto","description":"Looks for an input field called chatInput"},{"name":"Define below","value":"define","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"auto","displayOptions":{"hide":{"@version":[{"_cnd":{"lte":1.2}}],"agent":["sqlAgent"]}}},{"displayName":"Text","name":"text","type":"string","required":true,"default":"","placeholder":"e.g. Hello, how can you help me?","typeOptions":{"rows":2},"displayOptions":{"show":{"promptType":["define"]},"hide":{"agent":["sqlAgent"]}}},{"displayName":"Require Specific Output Format","name":"hasOutputParser","type":"boolean","default":false,"noDataExpression":true,"displayOptions":{"hide":{"@version":[{"_cnd":{"lte":1.2}}],"agent":["sqlAgent"]}}},{"displayName":"Connect an <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_outputParser'>output parser</a> on the canvas to specify the output format you require","name":"notice","type":"notice","default":"","displayOptions":{"show":{"hasOutputParser":[true]}}},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["toolsAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"System Message","name":"systemMessage","type":"string","default":"You are a helpful assistant","description":"The message that will be sent to the agent before the conversation starts","typeOptions":{"rows":6}},{"displayName":"Max Iterations","name":"maxIterations","type":"number","default":10,"description":"The maximum number of iterations the agent will run before stopping"},{"displayName":"Return Intermediate Steps","name":"returnIntermediateSteps","type":"boolean","default":false,"description":"Whether or not the output should include intermediate steps the agent took"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["conversationalAgent"],"@version":[1]}},"default":"={{ $json.input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["conversationalAgent"],"@version":[1.1]}},"default":"={{ $json.chat_input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["conversationalAgent"],"@version":[1.2]}},"default":"={{ $json.chatInput }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["conversationalAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Human Message","name":"humanMessage","type":"string","default":"TOOLS\n------\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n\n{tools}\n\n{format_instructions}\n\nUSER'S INPUT\n--------------------\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n\n{{input}}","description":"The message that will provide the agent with a list of tools to use","typeOptions":{"rows":6}},{"displayName":"System Message","name":"systemMessage","type":"string","default":"Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.","description":"The message that will be sent to the agent before the conversation starts","typeOptions":{"rows":6}},{"displayName":"Max Iterations","name":"maxIterations","type":"number","default":10,"description":"The maximum number of iterations the agent will run before stopping"},{"displayName":"Return Intermediate Steps","name":"returnIntermediateSteps","type":"boolean","default":false,"description":"Whether or not the output should include intermediate steps the agent took"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["openAiFunctionsAgent"],"@version":[1]}},"default":"={{ $json.input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["openAiFunctionsAgent"],"@version":[1.1]}},"default":"={{ $json.chat_input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["openAiFunctionsAgent"],"@version":[1.2]}},"default":"={{ $json.chatInput }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["openAiFunctionsAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"System Message","name":"systemMessage","type":"string","default":"You are a helpful AI assistant.","description":"The message that will be sent to the agent before the conversation starts","typeOptions":{"rows":6}},{"displayName":"Max Iterations","name":"maxIterations","type":"number","default":10,"description":"The maximum number of iterations the agent will run before stopping"},{"displayName":"Return Intermediate Steps","name":"returnIntermediateSteps","type":"boolean","default":false,"description":"Whether or not the output should include intermediate steps the agent took"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["reActAgent"],"@version":[1]}},"default":"={{ $json.input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["reActAgent"],"@version":[1.1]}},"default":"={{ $json.chat_input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["reActAgent"],"@version":[1.2]}},"default":"={{ $json.chatInput }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["reActAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Human Message Template","name":"humanMessageTemplate","type":"string","default":"{input}\n\n{agent_scratchpad}","description":"String to use directly as the human message template","typeOptions":{"rows":6}},{"displayName":"Prefix Message","name":"prefix","type":"string","default":"Answer the following questions as best you can. You have access to the following tools:","description":"String to put before the list of tools","typeOptions":{"rows":6}},{"displayName":"Suffix Message for Chat Model","name":"suffixChat","type":"string","default":"Begin! Reminder to always use the exact characters `Final Answer` when responding.","description":"String to put after the list of tools that will be used if chat model is used","typeOptions":{"rows":6}},{"displayName":"Suffix Message for Regular Model","name":"suffix","type":"string","default":"Begin!\n\n\tQuestion: {input}\n\tThought:{agent_scratchpad}","description":"String to put after the list of tools that will be used if regular model is used","typeOptions":{"rows":6}},{"displayName":"Return Intermediate Steps","name":"returnIntermediateSteps","type":"boolean","default":false,"description":"Whether or not the output should include intermediate steps the agent took"}]},{"displayName":"Data Source","name":"dataSource","type":"options","displayOptions":{"show":{"agent":["sqlAgent"],"@version":[{"_cnd":{"lt":1.4}}]}},"default":"sqlite","description":"SQL database to connect to","options":[{"name":"MySQL","value":"mysql","description":"Connect to a MySQL database"},{"name":"Postgres","value":"postgres","description":"Connect to a Postgres database"},{"name":"SQLite","value":"sqlite","description":"Use SQLite by connecting a database file as binary input"}]},{"displayName":"Data Source","name":"dataSource","type":"options","displayOptions":{"show":{"agent":["sqlAgent"],"@version":[{"_cnd":{"gte":1.4}}]}},"default":"postgres","description":"SQL database to connect to","options":[{"name":"MySQL","value":"mysql","description":"Connect to a MySQL database"},{"name":"Postgres","value":"postgres","description":"Connect to a Postgres database"},{"name":"SQLite","value":"sqlite","description":"Use SQLite by connecting a database file as binary input"}]},{"displayName":"Credentials","name":"credentials","type":"credentials","default":""},{"displayName":"Pass the SQLite database into this node as binary data, e.g. by inserting a 'Read/Write Files from Disk' node beforehand","name":"sqLiteFileNotice","type":"notice","default":"","displayOptions":{"show":{"agent":["sqlAgent"],"dataSource":["sqlite"]}}},{"displayName":"Input Binary Field","name":"binaryPropertyName","type":"string","default":"data","required":true,"placeholder":"e.g data","hint":"The name of the input binary field containing the file to be extracted","displayOptions":{"show":{"agent":["sqlAgent"],"dataSource":["sqlite"]}}},{"displayName":"Prompt","name":"input","type":"string","displayOptions":{"show":{"agent":["sqlAgent"],"@version":[{"_cnd":{"lte":1.2}}]}},"default":"","required":true,"typeOptions":{"rows":5}},{"displayName":"Prompt","name":"promptType","type":"options","options":[{"name":"Take from previous node automatically","value":"auto","description":"Looks for an input field called chatInput"},{"name":"Define below","value":"define","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"auto","displayOptions":{"hide":{"@version":[{"_cnd":{"lte":1.2}}]},"show":{"agent":["sqlAgent"]}}},{"displayName":"Text","name":"text","type":"string","required":true,"default":"","placeholder":"e.g. Hello, how can you help me?","typeOptions":{"rows":2},"displayOptions":{"show":{"promptType":["define"],"agent":["sqlAgent"]}}},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["sqlAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Ignored Tables","name":"ignoredTables","type":"string","default":"","description":"Comma-separated list of tables to ignore from the database. If empty, no tables are ignored."},{"displayName":"Include Sample Rows","name":"includedSampleRows","type":"number","description":"Number of sample rows to include in the prompt to the agent. It helps the agent to understand the schema of the database but it also increases the amount of tokens used.","default":3},{"displayName":"Included Tables","name":"includedTables","type":"string","default":"","description":"Comma-separated list of tables to include in the database. If empty, all tables are included."},{"displayName":"Prefix Prompt","name":"prefixPrompt","type":"string","default":"You are an agent designed to interact with an SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\nUnless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results using the LIMIT clause.\nYou can order the results by a relevant column to return the most interesting examples in the database.\nNever query for all the columns from a specific table, only ask for a the few relevant columns given the question.\nYou have access to tools for interacting with the database.\nOnly use the below tools. Only use the information returned by the below tools to construct your final answer.\nYou MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\nIf the question does not seem related to the database, just return \"I don't know\" as the answer.","description":"Prefix prompt to use for the agent","typeOptions":{"rows":10}},{"displayName":"Suffix Prompt","name":"suffixPrompt","type":"string","default":"Begin!\nChat History:\n{chatHistory}\n\nQuestion: {input}\nThought: I should look at the tables in the database to see what I can query.\n{agent_scratchpad}","description":"Suffix prompt to use for the agent","typeOptions":{"rows":4}},{"displayName":"Limit","name":"topK","type":"number","default":10,"description":"The maximum number of results to return"}]},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["planAndExecuteAgent"],"@version":[1]}},"default":"={{ $json.input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["planAndExecuteAgent"],"@version":[1.1]}},"default":"={{ $json.chat_input }}"},{"displayName":"Text","name":"text","type":"string","required":true,"displayOptions":{"show":{"agent":["planAndExecuteAgent"],"@version":[1.2]}},"default":"={{ $json.chatInput }}"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"agent":["planAndExecuteAgent"]}},"default":{},"placeholder":"Add Option","options":[{"displayName":"Human Message Template","name":"humanMessageTemplate","type":"string","default":"Previous steps: {previous_steps}\n\nCurrent objective: {current_step}\n\n{agent_scratchpad}\n\nYou may extract and combine relevant data from your previous steps when responding to me.","description":"The message that will be sent to the agent during each step execution","typeOptions":{"rows":6}}]}]},
{"displayName":"OpenAI Assistant","name":"@n8n/n8n-nodes-langchain.openAiAssistant","hidden":true,"icon":"fa:robot","group":["transform"],"version":[1,1.1],"description":"Utilizes Assistant API from Open AI.","subtitle":"Open AI Assistant","defaults":{"name":"OpenAI Assistant","color":"#404040"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Agents","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.openaiassistant/"}]}},"inputs":[{"type":"main"},{"type":"ai_tool","displayName":"Tools"}],"outputs":["main"],"credentials":[{"name":"openAiApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"Operation","name":"mode","type":"options","noDataExpression":true,"default":"existing","options":[{"name":"Use New Assistant","value":"new"},{"name":"Use Existing Assistant","value":"existing"}]},{"displayName":"Name","name":"name","type":"string","default":"","required":true,"displayOptions":{"show":{"/mode":["new"]}}},{"displayName":"Instructions","name":"instructions","type":"string","description":"How the Assistant and model should behave or respond","default":"","typeOptions":{"rows":5},"displayOptions":{"show":{"/mode":["new"]}}},{"displayName":"Model","name":"model","type":"options","description":"The model which will be used to power the assistant. <a href=\"https://beta.openai.com/docs/models/overview\">Learn more</a>. The Retrieval tool requires gpt-3.5-turbo-1106 and gpt-4-1106-preview models.","required":true,"displayOptions":{"show":{"/mode":["new"]}},"typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"gpt-3.5-turbo-1106"},{"displayName":"Assistant","name":"assistantId","type":"options","noDataExpression":true,"displayOptions":{"show":{"/mode":["existing"]}},"description":"The assistant to use. <a href=\"https://beta.openai.com/docs/assistants/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","headers":{"OpenAI-Beta":"assistants=v1"},"url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/assistants"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.id}}","description":"={{$responseItem.model}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"assistant"}},"required":true,"default":""},{"displayName":"Text","name":"text","type":"string","required":true,"default":"={{ $json.chat_input }}","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Text","name":"text","type":"string","required":true,"default":"={{ $json.chatInput }}","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"OpenAI Tools","name":"nativeTools","type":"multiOptions","default":[],"options":[{"name":"Code Interpreter","value":"code_interpreter"},{"name":"Knowledge Retrieval","value":"retrieval"}]},{"displayName":"Connect your own custom tools to this node on the canvas","name":"noticeTools","type":"notice","default":""},{"displayName":"Upload files for retrieval using the <a href=\"https://platform.openai.com/playground\" target=\"_blank\">OpenAI website<a/>","name":"noticeTools","type":"notice","typeOptions":{"noticeTheme":"info"},"displayOptions":{"show":{"/nativeTools":["retrieval"]}},"default":""},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Timeout","name":"timeout","default":10000,"description":"Maximum amount of time a request is allowed to take in milliseconds","type":"number"}]}]},
{"displayName":"Summarization Chain","name":"@n8n/n8n-nodes-langchain.chainSummarization","icon":"fa:link","group":["transform"],"description":"Transforms text into a concise summary","codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainsummarization/"}]}},"defaultVersion":2,"version":[2],"defaults":{"name":"Summarization Chain","color":"#909298"},"inputs":"={{ ((parameter) => { function getInputs(parameters) {\n    const chunkingMode = parameters === null || parameters === void 0 ? void 0 : parameters.chunkingMode;\n    const operationMode = parameters === null || parameters === void 0 ? void 0 : parameters.operationMode;\n    const inputs = [\n        { displayName: '', type: \"main\" },\n        {\n            displayName: 'Model',\n            maxConnections: 1,\n            type: \"ai_languageModel\",\n            required: true,\n        },\n    ];\n    if (operationMode === 'documentLoader') {\n        inputs.push({\n            displayName: 'Document',\n            type: \"ai_document\",\n            required: true,\n            maxConnections: 1,\n        });\n        return inputs;\n    }\n    if (chunkingMode === 'advanced') {\n        inputs.push({\n            displayName: 'Text Splitter',\n            type: \"ai_textSplitter\",\n            required: false,\n            maxConnections: 1,\n        });\n        return inputs;\n    }\n    return inputs;\n}; return getInputs(parameter) })($parameter) }}","outputs":["main"],"credentials":[],"properties":[{"displayName":"Save time with an <a href=\"/templates/1951\" target=\"_blank\">example</a> of how this node works","name":"notice","type":"notice","default":""},{"displayName":"Data to Summarize","name":"operationMode","noDataExpression":true,"type":"options","description":"How to pass data into the summarization chain","default":"nodeInputJson","options":[{"name":"Use Node Input (JSON)","value":"nodeInputJson","description":"Summarize the JSON data coming into this node from the previous one"},{"name":"Use Node Input (Binary)","value":"nodeInputBinary","description":"Summarize the binary data coming into this node from the previous one"},{"name":"Use Document Loader","value":"documentLoader","description":"Use a loader sub-node with more configuration options"}]},{"displayName":"Chunking Strategy","name":"chunkingMode","noDataExpression":true,"type":"options","description":"Chunk splitting strategy","default":"simple","options":[{"name":"Simple (Define Below)","value":"simple"},{"name":"Advanced","value":"advanced","description":"Use a splitter sub-node with more configuration options"}],"displayOptions":{"show":{"/operationMode":["nodeInputJson","nodeInputBinary"]}}},{"displayName":"Characters Per Chunk","name":"chunkSize","description":"Controls the max size (in terms of number of characters) of the final document chunk","type":"number","default":1000,"displayOptions":{"show":{"/chunkingMode":["simple"]}}},{"displayName":"Chunk Overlap (Characters)","name":"chunkOverlap","type":"number","description":"Specifies how much characters overlap there should be between chunks","default":200,"displayOptions":{"show":{"/chunkingMode":["simple"]}}},{"displayName":"Options","name":"options","type":"collection","default":{},"placeholder":"Add Option","options":[{"displayName":"Input Data Field Name","name":"binaryDataKey","type":"string","default":"data","description":"The name of the field in the agent or chains input that contains the binary file to be processed","displayOptions":{"show":{"/operationMode":["nodeInputBinary"]}}},{"displayName":"Summarization Method and Prompts","name":"summarizationMethodAndPrompts","type":"fixedCollection","default":{"values":{"summarizationMethod":"map_reduce","prompt":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","combineMapPrompt":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:"}},"placeholder":"Add Option","typeOptions":{},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Summarization Method","name":"summarizationMethod","type":"options","description":"The type of summarization to run","default":"map_reduce","options":[{"name":"Map Reduce (Recommended)","value":"map_reduce","description":"Summarize each document (or chunk) individually, then summarize those summaries"},{"name":"Refine","value":"refine","description":"Summarize the first document (or chunk). Then update that summary based on the next document (or chunk), and repeat."},{"name":"Stuff","value":"stuff","description":"Pass all documents (or chunks) at once. Ideal for small datasets."}]},{"displayName":"Individual Summary Prompt","name":"combineMapPrompt","type":"string","hint":"The prompt to summarize an individual document (or chunk)","displayOptions":{"hide":{"/options.summarizationMethodAndPrompts.values.summarizationMethod":["stuff","refine"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","typeOptions":{"rows":9}},{"displayName":"Final Prompt to Combine","name":"prompt","type":"string","default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","hint":"The prompt to combine individual summaries","displayOptions":{"hide":{"/options.summarizationMethodAndPrompts.values.summarizationMethod":["stuff","refine"]}},"typeOptions":{"rows":9}},{"displayName":"Prompt","name":"prompt","type":"string","default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","displayOptions":{"hide":{"/options.summarizationMethodAndPrompts.values.summarizationMethod":["refine","map_reduce"]}},"typeOptions":{"rows":9}},{"displayName":"Subsequent (Refine) Prompt","name":"refinePrompt","type":"string","displayOptions":{"hide":{"/options.summarizationMethodAndPrompts.values.summarizationMethod":["stuff","map_reduce"]}},"default":"Your job is to produce a final summary\nWe have provided an existing summary up to a certain point: \"{existing_answer}\"\nWe have the opportunity to refine the existing summary\n(only if needed) with some more context below.\n------------\n\"{text}\"\n------------\n\nGiven the new context, refine the original summary\nIf the context isn't useful, return the original summary.\n\nREFINED SUMMARY:","hint":"The prompt to refine the summary based on the next document (or chunk)","typeOptions":{"rows":9}},{"displayName":"Initial Prompt","name":"refineQuestionPrompt","type":"string","displayOptions":{"hide":{"/options.summarizationMethodAndPrompts.values.summarizationMethod":["stuff","map_reduce"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","hint":"The prompt for the first document (or chunk)","typeOptions":{"rows":9}}]}]}]}]},
{"displayName":"Summarization Chain","name":"@n8n/n8n-nodes-langchain.chainSummarization","icon":"fa:link","group":["transform"],"description":"Transforms text into a concise summary","codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainsummarization/"}]}},"defaultVersion":2,"version":1,"defaults":{"name":"Summarization Chain","color":"#909298"},"inputs":["main",{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true}],"outputs":["main"],"credentials":[],"properties":[{"displayName":"Save time with an <a href=\"/templates/1951\" target=\"_blank\">example</a> of how this node works","name":"notice","type":"notice","default":""},{"displayName":"Type","name":"type","type":"options","description":"The type of summarization to run","default":"map_reduce","options":[{"name":"Map Reduce (Recommended)","value":"map_reduce","description":"Summarize each document (or chunk) individually, then summarize those summaries"},{"name":"Refine","value":"refine","description":"Summarize the first document (or chunk). Then update that summary based on the next document (or chunk), and repeat."},{"name":"Stuff","value":"stuff","description":"Pass all documents (or chunks) at once. Ideal for small datasets."}]},{"displayName":"Options","name":"options","type":"collection","default":{},"placeholder":"Add Option","options":[{"displayName":"Final Prompt to Combine","name":"combineMapPrompt","type":"string","hint":"The prompt to combine individual summaries","displayOptions":{"show":{"/type":["map_reduce"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","typeOptions":{"rows":6}},{"displayName":"Individual Summary Prompt","name":"prompt","type":"string","default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","hint":"The prompt to summarize an individual document (or chunk)","displayOptions":{"show":{"/type":["map_reduce"]}},"typeOptions":{"rows":6}},{"displayName":"Prompt","name":"prompt","type":"string","default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","displayOptions":{"show":{"/type":["stuff"]}},"typeOptions":{"rows":6}},{"displayName":"Subsequent (Refine) Prompt","name":"refinePrompt","type":"string","displayOptions":{"show":{"/type":["refine"]}},"default":"Your job is to produce a final summary\nWe have provided an existing summary up to a certain point: \"{existing_answer}\"\nWe have the opportunity to refine the existing summary\n(only if needed) with some more context below.\n------------\n\"{text}\"\n------------\n\nGiven the new context, refine the original summary\nIf the context isn't useful, return the original summary.\n\nREFINED SUMMARY:","hint":"The prompt to refine the summary based on the next document (or chunk)","typeOptions":{"rows":6}},{"displayName":"Initial Prompt","name":"refineQuestionPrompt","type":"string","displayOptions":{"show":{"/type":["refine"]}},"default":"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:","hint":"The prompt for the first document (or chunk)","typeOptions":{"rows":6}}]}]},
{"displayName":"Basic LLM Chain","name":"@n8n/n8n-nodes-langchain.chainLlm","icon":"fa:link","group":["transform"],"version":[1,1.1,1.2,1.3,1.4],"description":"A simple chain to prompt a large language model","defaults":{"name":"Basic LLM Chain","color":"#909298"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm/"}]}},"inputs":"={{ ((parameter) => { function getInputs(parameters) {\n    const hasOutputParser = parameters === null || parameters === void 0 ? void 0 : parameters.hasOutputParser;\n    const inputs = [\n        { displayName: '', type: \"main\" },\n        {\n            displayName: 'Model',\n            maxConnections: 1,\n            type: \"ai_languageModel\",\n            required: true,\n        },\n    ];\n    if (hasOutputParser === undefined || hasOutputParser === true) {\n        inputs.push({ displayName: 'Output Parser', type: \"ai_outputParser\" });\n    }\n    return inputs;\n}; return getInputs(parameter) })($parameter) }}","outputs":["main"],"credentials":[],"properties":[{"displayName":"Save time with an <a href=\"/templates/1978\" target=\"_blank\">example</a> of how this node works","name":"notice","type":"notice","default":""},{"displayName":"Prompt","name":"prompt","type":"string","required":true,"default":"={{ $json.input }}","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Prompt","name":"prompt","type":"string","required":true,"default":"={{ $json.chat_input }}","displayOptions":{"show":{"@version":[1.1,1.2]}}},{"displayName":"Prompt","name":"prompt","type":"string","required":true,"default":"={{ $json.chatInput }}","displayOptions":{"show":{"@version":[1.3]}}},{"displayName":"Prompt","name":"promptType","type":"options","options":[{"name":"Take from previous node automatically","value":"auto","description":"Looks for an input field called chatInput"},{"name":"Define below","value":"define","description":"Use an expression to reference data in previous nodes or enter static text"}],"displayOptions":{"hide":{"@version":[1,1.1,1.2,1.3]}},"default":"auto"},{"displayName":"Text","name":"text","type":"string","required":true,"default":"","placeholder":"e.g. Hello, how can you help me?","typeOptions":{"rows":2},"displayOptions":{"show":{"promptType":["define"]}}},{"displayName":"Require Specific Output Format","name":"hasOutputParser","type":"boolean","default":false,"noDataExpression":true,"displayOptions":{"hide":{"@version":[1,1.1,1.3]}}},{"displayName":"Chat Messages (if Using a Chat Model)","name":"messages","type":"fixedCollection","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add prompt","options":[{"name":"messageValues","displayName":"Prompt","values":[{"displayName":"Type Name or ID","name":"type","type":"options","options":[{"name":"AI","value":"AIMessagePromptTemplate"},{"name":"System","value":"SystemMessagePromptTemplate"},{"name":"User","value":"HumanMessagePromptTemplate"}],"default":"SystemMessagePromptTemplate"},{"displayName":"Message Type","name":"messageType","type":"options","displayOptions":{"show":{"type":["HumanMessagePromptTemplate"]}},"options":[{"name":"Text","value":"text","description":"Simple text message"},{"name":"Image (Binary)","value":"imageBinary","description":"Process the binary input from the previous node"},{"name":"Image (URL)","value":"imageUrl","description":"Process the image from the specified URL"}],"default":"text"},{"displayName":"Image Data Field Name","name":"binaryImageDataKey","type":"string","default":"data","required":true,"description":"The name of the field in the chains input that contains the binary image file to be processed","displayOptions":{"show":{"messageType":["imageBinary"]}}},{"displayName":"Image URL","name":"imageUrl","type":"string","default":"","required":true,"description":"URL to the image to be processed","displayOptions":{"show":{"messageType":["imageUrl"]}}},{"displayName":"Image Details","description":"Control how the model processes the image and generates its textual understanding","name":"imageDetail","type":"options","displayOptions":{"show":{"type":["HumanMessagePromptTemplate"],"messageType":["imageBinary","imageUrl"]}},"options":[{"name":"Auto","value":"auto","description":"Model will use the auto setting which will look at the image input size and decide if it should use the low or high setting"},{"name":"Low","value":"low","description":"The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail."},{"name":"High","value":"high","description":"Allows the model to see the low res image and then creates detailed crops of input images as 512px squares based on the input image size. Each of the detailed crops uses twice the token budget (65 tokens) for a total of 129 tokens."}],"default":"auto"},{"displayName":"Message","name":"message","type":"string","required":true,"displayOptions":{"hide":{"messageType":["imageBinary","imageUrl"]}},"default":""}]}]},{"displayName":"Connect an <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_outputParser'>output parser</a> on the canvas to specify the output format you require","name":"notice","type":"notice","default":"","displayOptions":{"show":{"hasOutputParser":[true]}}}]},
{"displayName":"Question and Answer Chain","name":"@n8n/n8n-nodes-langchain.chainRetrievalQa","icon":"fa:link","group":["transform"],"version":[1,1.1,1.2,1.3],"description":"Answer questions about retrieved documents","defaults":{"name":"Question and Answer Chain","color":"#909298"},"codex":{"alias":["LangChain"],"categories":["AI"],"subcategories":{"AI":["Chains","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/"}]}},"inputs":["main",{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":["main"],"credentials":[],"properties":[{"displayName":"Save time with an <a href=\"/templates/1960\" target=\"_blank\">example</a> of how this node works","name":"notice","type":"notice","default":""},{"displayName":"Query","name":"query","type":"string","required":true,"default":"={{ $json.input }}","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Query","name":"query","type":"string","required":true,"default":"={{ $json.chat_input }}","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Query","name":"query","type":"string","required":true,"default":"={{ $json.chatInput }}","displayOptions":{"show":{"@version":[1.2]}}},{"displayName":"Prompt","name":"promptType","type":"options","options":[{"name":"Take from previous node automatically","value":"auto","description":"Looks for an input field called chatInput"},{"name":"Define below","value":"define","description":"Use an expression to reference data in previous nodes or enter static text"}],"displayOptions":{"hide":{"@version":[{"_cnd":{"lte":1.2}}]}},"default":"auto"},{"displayName":"Text","name":"text","type":"string","required":true,"default":"","typeOptions":{"rows":2},"displayOptions":{"show":{"promptType":["define"]}}}]},
{"displayName":"LangChain Code","name":"@n8n/n8n-nodes-langchain.code","icon":"fa:code","group":["transform"],"version":1,"description":"LangChain Code Node","defaults":{"name":"LangChain Code"},"codex":{"categories":["AI"],"subcategories":{"AI":["Miscellaneous"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.code/"}]}},"inputs":"={{ ((values) => { const connectorTypes = {\"ai_chain\":\"Chain\",\"ai_document\":\"Document\",\"ai_embedding\":\"Embedding\",\"ai_languageModel\":\"Language Model\",\"ai_memory\":\"Memory\",\"ai_outputParser\":\"Output Parser\",\"ai_textSplitter\":\"Text Splitter\",\"ai_tool\":\"Tool\",\"ai_vectorStore\":\"Vector Store\",\"main\":\"Main\"}; return values.map(value => { return { type: value.type, required: value.required, maxConnections: value.maxConnections === -1 ? undefined : value.maxConnections, displayName: connectorTypes[value.type] !== 'Main' ? connectorTypes[value.type] : undefined } } ) })($parameter.inputs.input) }}","outputs":"={{ ((values) => { const connectorTypes = {\"ai_chain\":\"Chain\",\"ai_document\":\"Document\",\"ai_embedding\":\"Embedding\",\"ai_languageModel\":\"Language Model\",\"ai_memory\":\"Memory\",\"ai_outputParser\":\"Output Parser\",\"ai_textSplitter\":\"Text Splitter\",\"ai_tool\":\"Tool\",\"ai_vectorStore\":\"Vector Store\",\"main\":\"Main\"}; return values.map(value => { return { type: value.type, displayName: connectorTypes[value.type] !== 'Main' ? connectorTypes[value.type] : undefined } } ) })($parameter.outputs.output) }}","properties":[{"displayName":"Code","name":"code","placeholder":"Add Code","type":"fixedCollection","noDataExpression":true,"default":{},"options":[{"name":"execute","displayName":"Execute","values":[{"displayName":"JavaScript - Execute","name":"code","type":"string","typeOptions":{"editor":"jsEditor"},"default":"const { PromptTemplate } = require('@langchain/core/prompts');\n\nconst query = 'Tell me a joke';\nconst prompt = PromptTemplate.fromTemplate(query);\nconst llm = await this.getInputConnectionData('ai_languageModel', 0);\nlet chain = prompt.pipe(llm);\nconst output = await chain.invoke();\nreturn [ {json: { output } } ];","hint":"This code will only run and return data if a \"Main\" input & output got created.","noDataExpression":true}]},{"name":"supplyData","displayName":"Supply Data","values":[{"displayName":"JavaScript - Supply Data","name":"code","type":"string","typeOptions":{"editor":"jsEditor"},"default":"const { WikipediaQueryRun } = require('langchain/tools');\nreturn new WikipediaQueryRun();","hint":"This code will only run and return data if an output got created which is not \"Main\".","noDataExpression":true}]}]},{"displayName":"You can import LangChain and use all available functionality. Debug by using <code>console.log()</code> statements and viewing their output in the browser console.","name":"notice","type":"notice","default":""},{"displayName":"Inputs","name":"inputs","placeholder":"Add Input","type":"fixedCollection","noDataExpression":true,"typeOptions":{"multipleValues":true,"sortable":true},"description":"The input to add","default":{},"options":[{"name":"input","displayName":"Input","values":[{"displayName":"Type","name":"type","type":"options","options":[{"name":"Chain","value":"ai_chain"},{"name":"Document","value":"ai_document"},{"name":"Embedding","value":"ai_embedding"},{"name":"Language Model","value":"ai_languageModel"},{"name":"Memory","value":"ai_memory"},{"name":"Output Parser","value":"ai_outputParser"},{"name":"Text Splitter","value":"ai_textSplitter"},{"name":"Tool","value":"ai_tool"},{"name":"Vector Store","value":"ai_vectorStore"},{"name":"Main","value":"main"}],"noDataExpression":true,"default":"","required":true,"description":"The type of the input"},{"displayName":"Max Connections","name":"maxConnections","type":"number","noDataExpression":true,"default":-1,"required":true,"description":"How many nodes of this type are allowed to be connected. Set it to -1 for unlimited."},{"displayName":"Required","name":"required","type":"boolean","noDataExpression":true,"default":false,"required":true,"description":"Whether the input needs a connection"}]}]},{"displayName":"Outputs","name":"outputs","placeholder":"Add Output","type":"fixedCollection","noDataExpression":true,"typeOptions":{"multipleValues":true,"sortable":true},"description":"The output to add","default":{},"options":[{"name":"output","displayName":"Output","values":[{"displayName":"Type","name":"type","type":"options","options":[{"name":"Chain","value":"ai_chain"},{"name":"Document","value":"ai_document"},{"name":"Embedding","value":"ai_embedding"},{"name":"Language Model","value":"ai_languageModel"},{"name":"Memory","value":"ai_memory"},{"name":"Output Parser","value":"ai_outputParser"},{"name":"Text Splitter","value":"ai_textSplitter"},{"name":"Tool","value":"ai_tool"},{"name":"Vector Store","value":"ai_vectorStore"},{"name":"Main","value":"main"}],"noDataExpression":true,"default":"","required":true,"description":"The type of the input"}]}]}]},
{"displayName":"Default Data Loader","name":"@n8n/n8n-nodes-langchain.documentDefaultDataLoader","group":["transform"],"version":1,"description":"Load data from previous step in the workflow","defaults":{"name":"Default Data Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentdefaultdataloader/"}]}},"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter","required":true}],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"This will load data from a previous step in the workflow. <a href=\"/templates/1962\" target=\"_blank\">Example</a>","name":"notice","type":"notice","default":""},{"displayName":"Type of Data","name":"dataType","type":"options","default":"json","required":true,"noDataExpression":true,"options":[{"name":"JSON","value":"json","description":"Process JSON data from previous step in the workflow"},{"name":"Binary","value":"binary","description":"Process binary data from previous step in the workflow"}]},{"displayName":"Mode","name":"jsonMode","type":"options","default":"allInputData","required":true,"displayOptions":{"show":{"dataType":["json"]}},"options":[{"name":"Load All Input Data","value":"allInputData","description":"Use all JSON data that flows into the parent agent or chain"},{"name":"Load Specific Data","value":"expressionData","description":"Load a subset of data, and/or data from any previous step in the workflow"}]},{"displayName":"Data Format","name":"loader","type":"options","default":"auto","required":true,"displayOptions":{"show":{"dataType":["binary"]}},"options":[{"name":"Automatically Detect by Mime Type","value":"auto","description":"Uses the mime type to detect the format"},{"name":"CSV","value":"csvLoader","description":"Load CSV files"},{"name":"Docx","value":"docxLoader","description":"Load Docx documents"},{"name":"EPub","value":"epubLoader","description":"Load EPub files"},{"name":"JSON","value":"jsonLoader","description":"Load JSON files"},{"name":"PDF","value":"pdfLoader","description":"Load PDF documents"},{"name":"Text","value":"textLoader","description":"Load plain text files"}]},{"displayName":"Data","name":"jsonData","type":"string","typeOptions":{"rows":6},"default":"","required":true,"description":"Drag and drop fields from the input pane, or use an expression","displayOptions":{"show":{"dataType":["json"],"jsonMode":["expressionData"]}}},{"displayName":"Input Data Field Name","name":"binaryDataKey","type":"string","default":"data","required":true,"description":"The name of the field in the agent or chains input that contains the binary file to be processed","displayOptions":{"show":{"dataType":["binary"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"JSON Pointers","name":"pointers","type":"string","default":"","description":"Pointers to extract from JSON, e.g. \"/text\" or \"/text, /meta/title\"","displayOptions":{"show":{"/loader":["jsonLoader","auto"]}}},{"displayName":"CSV Separator","name":"separator","type":"string","description":"Separator to use for CSV","default":",","displayOptions":{"show":{"/loader":["csvLoader","auto"]}}},{"displayName":"CSV Column","name":"column","type":"string","default":"","description":"Column to extract from CSV","displayOptions":{"show":{"/loader":["csvLoader","auto"]}}},{"displayName":"Split Pages in PDF","description":"Whether to split PDF pages into separate documents","name":"splitPages","type":"boolean","default":true,"displayOptions":{"show":{"/loader":["pdfLoader","auto"]}}},{"displayName":"Metadata","name":"metadata","type":"fixedCollection","description":"Metadata to add to each document. Could be used for filtering during retrieval","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add property","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentDefaultDataLoader/binary.svg"},
{"hidden":true,"displayName":"Binary Input Loader","name":"@n8n/n8n-nodes-langchain.documentBinaryInputLoader","group":["transform"],"version":1,"description":"Use binary data from a previous step in the workflow","defaults":{"name":"Binary Input Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentdefaultdataloader/"}]}},"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter","required":true}],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Loader Type","name":"loader","type":"options","default":"jsonLoader","required":true,"options":[{"name":"CSV Loader","value":"csvLoader","description":"Load CSV files"},{"name":"Docx Loader","value":"docxLoader","description":"Load Docx documents"},{"name":"EPub Loader","value":"epubLoader","description":"Load EPub files"},{"name":"JSON Loader","value":"jsonLoader","description":"Load JSON files"},{"name":"PDF Loader","value":"pdfLoader","description":"Load PDF documents"},{"name":"Text Loader","value":"textLoader","description":"Load plain text files"}]},{"displayName":"Binary Data Key","name":"binaryDataKey","type":"string","default":"data","required":true,"description":"Name of the binary property from which to read the file buffer"},{"displayName":"Split Pages","name":"splitPages","type":"boolean","default":true,"displayOptions":{"show":{"loader":["pdfLoader"]}}},{"displayName":"Column","name":"column","type":"string","default":"","description":"Column to extract from CSV","displayOptions":{"show":{"loader":["csvLoader"]}}},{"displayName":"Separator","name":"separator","type":"string","description":"Separator to use for CSV","default":",","displayOptions":{"show":{"loader":["csvLoader"]}}},{"displayName":"Pointers","name":"pointers","type":"string","default":"","description":"Pointers to extract from JSON, e.g. \"/text\" or \"/text, /meta/title\"","displayOptions":{"show":{"loader":["jsonLoader"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata","name":"metadata","type":"fixedCollection","description":"Metadata to add to each document. Could be used for filtering during retrieval","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add property","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentBinaryInputLoader/binary.svg"},
{"displayName":"GitHub Document Loader","name":"@n8n/n8n-nodes-langchain.documentGithubLoader","group":["transform"],"version":1,"description":"Use GitHub data as input to this chain","defaults":{"name":"GitHub Document Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentgithubloader/"}]}},"credentials":[{"name":"githubApi","required":true}],"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter"}],"inputNames":["Text Splitter"],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Repository Link","name":"repository","type":"string","default":""},{"displayName":"Branch","name":"branch","type":"string","default":"main"},{"displayName":"Options","name":"additionalOptions","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Recursive","name":"recursive","type":"boolean","default":false},{"displayName":"Ignore Paths","name":"ignorePaths","type":"string","description":"Comma-separated list of paths to ignore, e.g. \"docs, src/tests","default":""}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentGithubLoader/github.svg"},
{"hidden":true,"displayName":"JSON Input Loader","name":"@n8n/n8n-nodes-langchain.documentJsonInputLoader","group":["transform"],"version":1,"description":"Use JSON data from a previous step in the workflow","defaults":{"name":"JSON Input Loader"},"codex":{"categories":["AI"],"subcategories":{"AI":["Document Loaders"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.documentdefaultdataloader/"}]}},"inputs":[{"displayName":"Text Splitter","maxConnections":1,"type":"ai_textSplitter"}],"inputNames":["Text Splitter"],"outputs":["ai_document"],"outputNames":["Document"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Pointers","name":"pointers","type":"string","default":"","description":"Pointers to extract from JSON, e.g. \"/text\" or \"/text, /meta/title\""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata","name":"metadata","type":"fixedCollection","description":"Metadata to add to each document. Could be used for filtering during retrieval","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add property","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/document_loaders/DocumentJSONInputLoader/json.svg"},
{"displayName":"Embeddings Cohere","name":"@n8n/n8n-nodes-langchain.embeddingsCohere","group":["transform"],"version":1,"description":"Use Cohere Embeddings","defaults":{"name":"Embeddings Cohere"},"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"credentials":[{"name":"cohereApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingscohere/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the embeddings. <a href=\"https://docs.cohere.com/docs/models\">Learn more</a>.","default":"embed-english-v2.0","options":[{"name":"Embed-English-v2.0(4096 Dimensions)","value":"embed-english-v2.0"},{"name":"Embed-English-Light-v2.0(1024 Dimensions)","value":"embed-english-light-v2.0"},{"name":"Embed-Multilingual-v2.0(768 Dimensions)","value":"embed-multilingual-v2.0"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsCohere/cohere.svg"},
{"displayName":"Embeddings AWS Bedrock","name":"@n8n/n8n-nodes-langchain.embeddingsAwsBedrock","credentials":[{"name":"aws","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings AWS Bedrock","defaults":{"name":"Embeddings AWS Bedrock"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsawsbedrock/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"=https://bedrock.{{$credentials?.region ?? \"eu-central-1\"}}.amazonaws.com"},"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/foundation-models?byInferenceType=ON_DEMAND&byOutputModality=EMBEDDING"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"modelSummaries"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.modelName}}","description":"={{$responseItem.modelArn}}","value":"={{$responseItem.modelId}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":""},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsAwsBedrock/bedrock.svg"},
{"displayName":"Embeddings Azure OpenAI","name":"@n8n/n8n-nodes-langchain.embeddingsAzureOpenAi","credentials":[{"name":"azureOpenAiApi","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings Azure OpenAI","defaults":{"name":"Embeddings Azure OpenAI"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsazureopenai/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model (Deployment) Name","name":"model","type":"string","description":"The name of the model(deployment) to use","default":""},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Batch Size","name":"batchSize","default":512,"typeOptions":{"maxValue":2048},"description":"Maximum number of documents to send in each request","type":"number"},{"displayName":"Strip New Lines","name":"stripNewLines","default":true,"description":"Whether to strip new lines from the input text","type":"boolean"},{"displayName":"Timeout","name":"timeout","default":-1,"description":"Maximum amount of time a request is allowed to take in seconds. Set to -1 for no timeout.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsAzureOpenAi/azure.svg"},
{"displayName":"Embeddings Google PaLM","name":"@n8n/n8n-nodes-langchain.embeddingsGooglePalm","group":["transform"],"version":1,"description":"Use Google PaLM Embeddings","defaults":{"name":"Embeddings Google PaLM"},"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"credentials":[{"name":"googlePalmApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsgooglepalm/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the embeddings. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/embedding') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/embedding-gecko-001"},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsGooglePalm/google.svg"},
{"displayName":"Embeddings Google Gemini","name":"@n8n/n8n-nodes-langchain.embeddingsGoogleGemini","group":["transform"],"version":1,"description":"Use Google Gemini Embeddings","defaults":{"name":"Embeddings Google Gemini"},"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"credentials":[{"name":"googlePalmApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsgooglegemini/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the embeddings. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.includes('embedding') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"textembedding-gecko-multilingual@latest"},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsGoogleGemini/google.svg"},
{"displayName":"Embeddings Hugging Face Inference","name":"@n8n/n8n-nodes-langchain.embeddingsHuggingFaceInference","group":["transform"],"version":1,"description":"Use HuggingFace Inference Embeddings","defaults":{"name":"Embeddings HuggingFace Inference"},"credentials":[{"name":"huggingFaceApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingshuggingfaceinference/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.","name":"notice","type":"notice","default":""},{"displayName":"Model Name","name":"modelName","type":"string","default":"sentence-transformers/distilbert-base-nli-mean-tokens","description":"The model name to use from HuggingFace library"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Custom Inference Endpoint","name":"endpointUrl","default":"","description":"Custom endpoint URL","type":"string"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsHuggingFaceInference/huggingface.svg"},
{"displayName":"Embeddings Mistral Cloud","name":"@n8n/n8n-nodes-langchain.embeddingsMistralCloud","credentials":[{"name":"mistralCloudApi","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings Mistral Cloud","defaults":{"name":"Embeddings Mistral Cloud"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsmistralcloud/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"https://api.mistral.ai/v1"},"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","description":"The model which will compute the embeddings. <a href=\"https://docs.mistral.ai/platform/endpoints/\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.includes('embed') }}"}},{"type":"setKeyValue","properties":{"name":"={{ $responseItem.id }}","value":"={{ $responseItem.id }}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"mistral-embed"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Batch Size","name":"batchSize","default":512,"typeOptions":{"maxValue":2048},"description":"Maximum number of documents to send in each request","type":"number"},{"displayName":"Strip New Lines","name":"stripNewLines","default":true,"description":"Whether to strip new lines from the input text","type":"boolean"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsMistralCloud/mistral.svg"},
{"displayName":"Embeddings OpenAI","name":"@n8n/n8n-nodes-langchain.embeddingsOpenAi","credentials":[{"name":"openAiApi","required":true}],"group":["transform"],"version":1,"description":"Use Embeddings OpenAI","defaults":{"name":"Embeddings OpenAI"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsopenai/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the embeddings. <a href=\"https://platform.openai.com/docs/models/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.includes('embed') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"text-embedding-ada-002","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the embeddings. <a href=\"https://platform.openai.com/docs/models/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.includes('embed') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"text-embedding-3-small","displayOptions":{"hide":{"@version":[1]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Batch Size","name":"batchSize","default":512,"typeOptions":{"maxValue":2048},"description":"Maximum number of documents to send in each request","type":"number"},{"displayName":"Strip New Lines","name":"stripNewLines","default":true,"description":"Whether to strip new lines from the input text","type":"boolean"},{"displayName":"Timeout","name":"timeout","default":-1,"description":"Maximum amount of time a request is allowed to take in seconds. Set to -1 for no timeout.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":{"light":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsOpenAI/openAiLight.svg","dark":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsOpenAI/openAiLight.dark.svg"}},
{"displayName":"Embeddings Ollama","name":"@n8n/n8n-nodes-langchain.embeddingsOllama","group":["transform"],"version":1,"description":"Use Ollama Embeddings","defaults":{"name":"Embeddings Ollama"},"credentials":[{"name":"ollamaApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.baseUrl.replace(new RegExp(\"/$\"), \"\") }}"},"codex":{"categories":["AI"],"subcategories":{"AI":["Embeddings"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.embeddingsollama/"}]}},"inputs":[],"outputs":["ai_embedding"],"outputNames":["Embeddings"],"properties":[{"displayName":"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","default":"llama2","description":"The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/api/tags"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"required":true},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/embeddings/EmbeddingsOllama/ollama.svg"},
{"displayName":"Anthropic Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatAnthropic","group":["transform"],"version":[1,1.1],"description":"Language Model Anthropic","defaults":{"name":"Anthropic Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatanthropic/"}]},"alias":["claude","sonnet","opus"]},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"anthropicApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","options":[{"name":"Claude 3 Opus(20240229)","value":"claude-3-opus-20240229"},{"name":"Claude 3 Sonnet(20240229)","value":"claude-3-sonnet-20240229"},{"name":"Claude 3 Haiku(20240307)","value":"claude-3-haiku-20240307"},{"name":"LEGACY: Claude 2","value":"claude-2"},{"name":"LEGACY: Claude 2.1","value":"claude-2.1"},{"name":"LEGACY: Claude Instant 1.2","value":"claude-instant-1.2"},{"name":"LEGACY: Claude Instant 1","value":"claude-instant-1"}],"description":"The model which will generate the completion. <a href=\"https://docs.anthropic.com/claude/docs/models-overview\">Learn more</a>.","default":"claude-2","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Model","name":"model","type":"options","options":[{"name":"Claude 3 Opus(20240229)","value":"claude-3-opus-20240229"},{"name":"Claude 3 Sonnet(20240229)","value":"claude-3-sonnet-20240229"},{"name":"Claude 3 Haiku(20240307)","value":"claude-3-haiku-20240307"},{"name":"LEGACY: Claude 2","value":"claude-2"},{"name":"LEGACY: Claude 2.1","value":"claude-2.1"},{"name":"LEGACY: Claude Instant 1.2","value":"claude-instant-1.2"},{"name":"LEGACY: Claude Instant 1","value":"claude-instant-1"}],"description":"The model which will generate the completion. <a href=\"https://docs.anthropic.com/claude/docs/models-overview\">Learn more</a>.","default":"claude-3-sonnet-20240229","displayOptions":{"hide":{"@version":[1]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokensToSample","default":4096,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatAnthropic/anthropic.svg"},
{"displayName":"Azure OpenAI Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatAzureOpenAi","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"Azure OpenAI Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatazureopenai/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"azureOpenAiApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model (Deployment) Name","name":"model","type":"string","description":"The name of the model(deployment) to use","default":""},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Timeout","name":"timeout","default":60000,"description":"Maximum amount of time a request is allowed to take in milliseconds","type":"number"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatAzureOpenAi/azure.svg"},
{"displayName":"Google PaLM Language Model","name":"@n8n/n8n-nodes-langchain.lmGooglePalm","group":["transform"],"version":1,"description":"Language Model Google PaLM","defaults":{"name":"Google PaLM Language Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmgooglepalm/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"googlePalmApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the completion. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/text') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/text-bison-001"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxOutputTokens","default":1024,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":40,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":0.9,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmGooglePalm/google.svg"},
{"displayName":"AWS Bedrock Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatAwsBedrock","group":["transform"],"version":1,"description":"Language Model AWS Bedrock","defaults":{"name":"AWS Bedrock Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatawsbedrock/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"aws","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"=https://bedrock.{{$credentials?.region ?? \"eu-central-1\"}}.amazonaws.com"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/foundation-models?&byOutputModality=TEXT&byInferenceType=ON_DEMAND"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"modelSummaries"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.modelName}}","description":"={{$responseItem.modelArn}}","value":"={{$responseItem.modelId}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":""},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokensToSample","default":2000,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatAwsBedrock/bedrock.svg"},
{"displayName":"Google PaLM Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatGooglePalm","group":["transform"],"version":1,"description":"Chat Model Google PaLM","defaults":{"name":"Google PaLM Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglepalm/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"googlePalmApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the completion. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta3/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ $responseItem.name.startsWith('models/chat') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/chat-bison-001"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":40,"typeOptions":{"maxValue":1,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":0.9,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatGooglePalm/google.svg"},
{"displayName":"Google Gemini Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatGoogleGemini","group":["transform"],"version":1,"description":"Chat Model Google Gemini","defaults":{"name":"Google Gemini Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgooglegemini/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"googlePalmApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.host }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"modelName","type":"options","description":"The model which will generate the completion. <a href=\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/v1beta/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"filter","properties":{"pass":"={{ !$responseItem.name.includes('embedding') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}","description":"={{$responseItem.description}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"models/gemini-1.0-pro"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxOutputTokens","default":2048,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.4,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":32,"typeOptions":{"maxValue":40,"minValue":-1,"numberPrecision":1},"description":"Used to remove \"long tail\" low probability responses. Defaults to -1, which disables it.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Safety Settings","name":"safetySettings","type":"fixedCollection","typeOptions":{"multipleValues":true},"default":{"values":{"category":"HARM_CATEGORY_HARASSMENT","threshold":"HARM_BLOCK_THRESHOLD_UNSPECIFIED"}},"placeholder":"Add Option","options":[{"name":"values","displayName":"Values","values":[{"displayName":"Safety Category","name":"category","type":"options","description":"The category of harmful content to block","default":"HARM_CATEGORY_UNSPECIFIED","options":[{"value":"HARM_CATEGORY_HARASSMENT","name":"HARM_CATEGORY_HARASSMENT","description":"Harassment content"},{"value":"HARM_CATEGORY_HATE_SPEECH","name":"HARM_CATEGORY_HATE_SPEECH","description":"Hate speech and content"},{"value":"HARM_CATEGORY_SEXUALLY_EXPLICIT","name":"HARM_CATEGORY_SEXUALLY_EXPLICIT","description":"Sexually explicit content"},{"value":"HARM_CATEGORY_DANGEROUS_CONTENT","name":"HARM_CATEGORY_DANGEROUS_CONTENT","description":"Dangerous content"}]},{"displayName":"Safety Threshold","name":"threshold","type":"options","description":"The threshold of harmful content to block","default":"HARM_BLOCK_THRESHOLD_UNSPECIFIED","options":[{"value":"HARM_BLOCK_THRESHOLD_UNSPECIFIED","name":"HARM_BLOCK_THRESHOLD_UNSPECIFIED","description":"Threshold is unspecified"},{"value":"BLOCK_LOW_AND_ABOVE","name":"BLOCK_LOW_AND_ABOVE","description":"Content with NEGLIGIBLE will be allowed"},{"value":"BLOCK_MEDIUM_AND_ABOVE","name":"BLOCK_MEDIUM_AND_ABOVE","description":"Content with NEGLIGIBLE and LOW will be allowed"},{"value":"BLOCK_ONLY_HIGH","name":"BLOCK_ONLY_HIGH","description":"Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed"},{"value":"BLOCK_NONE","name":"BLOCK_NONE","description":"All content will be allowed"}]}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatGoogleGemini/google.svg"},
{"displayName":"Groq Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatGroq","group":["transform"],"version":1,"description":"Language Model Groq","defaults":{"name":"Groq Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatgroq/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"groqApi","required":true}],"requestDefaults":{"baseURL":"https://api.groq.com/openai/v1"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.active === true && $responseItem.object === \"model\" }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"description":"The model which will generate the completion. <a href=\"https://console.groq.com/docs/models\">Learn more</a>.","default":"llama3-8b-8192"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokensToSample","default":4096,"description":"The maximum number of tokens to generate in the completion","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatGroq/groq.svg"},
{"displayName":"Mistral Cloud Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatMistralCloud","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"Mistral Cloud Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatmistralcloud/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"mistralCloudApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"https://api.mistral.ai/v1"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://docs.mistral.ai/platform/endpoints/\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ !$responseItem.id.includes('embed') }}"}},{"type":"setKeyValue","properties":{"name":"={{ $responseItem.id }}","value":"={{ $responseItem.id }}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"mistral-small"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"},{"displayName":"Enable Safe Mode","name":"safeMode","default":false,"type":"boolean","description":"Whether to inject a safety prompt before all conversations"},{"displayName":"Random Seed","name":"randomSeed","type":"number","description":"The seed to use for random sampling. If set, different calls will generate deterministic results."}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LmChatMistralCloud/mistral.svg"},
{"displayName":"Ollama Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatOllama","group":["transform"],"version":1,"description":"Language Model Ollama","defaults":{"name":"Ollama Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"ollamaApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.baseUrl.replace(new RegExp(\"/$\"), \"\") }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","default":"llama2","description":"The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/api/tags"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"required":true},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":100,"minValue":-1,"numberPrecision":1},"description":"Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.","type":"number"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","type":"number","default":0,"typeOptions":{"minValue":0},"description":"Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition."},{"displayName":"Keep Alive","name":"keepAlive","type":"string","default":"5m","description":"Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes)."},{"displayName":"Low VRAM Mode","name":"lowVram","type":"boolean","default":false,"description":"Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory."},{"displayName":"Main GPU ID","name":"mainGpu","type":"number","default":0,"description":"Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs."},{"displayName":"Context Batch Size","name":"numBatch","type":"number","default":512,"description":"Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage."},{"displayName":"Context Length","name":"numCtx","type":"number","default":2048,"description":"The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model."},{"displayName":"Number of GPUs","name":"numGpu","type":"number","default":-1,"description":"Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection."},{"displayName":"Max Tokens to Generate","name":"numPredict","type":"number","default":-1,"description":"The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs."},{"displayName":"Number of CPU Threads","name":"numThread","type":"number","default":0,"description":"Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection."},{"displayName":"Penalize Newlines","name":"penalizeNewline","type":"boolean","default":true,"description":"Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text"},{"displayName":"Presence Penalty","name":"presencePenalty","type":"number","default":0,"description":"Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity."},{"displayName":"Repetition Penalty","name":"repeatPenalty","type":"number","default":1,"description":"Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty."},{"displayName":"Use Memory Locking","name":"useMLock","type":"boolean","default":false,"description":"Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory."},{"displayName":"Use Memory Mapping","name":"useMMap","type":"boolean","default":true,"description":"Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled."},{"displayName":"Load Vocabulary Only","name":"vocabOnly","type":"boolean","default":false,"description":"Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization."},{"displayName":"Output Format","name":"format","type":"options","options":[{"name":"Default","value":"default"},{"name":"JSON","value":"json"}],"default":"default","description":"Specifies the format of the API response"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatOllama/ollama.svg"},
{"displayName":"OpenAI Chat Model","name":"@n8n/n8n-nodes-langchain.lmChatOpenAi","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"OpenAI Chat Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"openAiApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"If using JSON response format, you must include word \"json\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.","name":"notice","type":"notice","default":"","displayOptions":{"show":{"/options.responseFormat":["json_object"]}}},{"displayName":"Model","name":"model","type":"options","description":"The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models/overview\">Learn more</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"={{ $parameter.options?.baseURL?.split(\"/\").slice(-1).pop() || \"v1\"  }}/models"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"data"}},{"type":"filter","properties":{"pass":"={{ $responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct') }}"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.id}}","value":"={{$responseItem.id}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"default":"gpt-3.5-turbo"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Response Format","name":"responseFormat","default":"text","type":"options","options":[{"name":"Text","value":"text","description":"Regular text response"},{"name":"JSON","value":"json_object","description":"Enables JSON mode, which should guarantee the message the model generates is valid JSON"}]},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Timeout","name":"timeout","default":60000,"description":"Maximum amount of time a request is allowed to take in milliseconds","type":"number"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":{"light":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatOpenAi/openAiLight.svg","dark":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMChatOpenAi/openAiLight.dark.svg"}},
{"displayName":"OpenAI Model","name":"@n8n/n8n-nodes-langchain.lmOpenAi","group":["transform"],"version":1,"description":"For advanced usage with an AI chain","defaults":{"name":"OpenAI Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmopenai/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"openAiApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $parameter.options?.baseURL?.split(\"/\").slice(0,-1).join(\"/\") || \"https://api.openai.com\" }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"resourceLocator","default":{"mode":"list","value":"gpt-3.5-turbo-instruct"},"required":true,"description":"The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models/overview\">Learn more</a>.","modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"openAiModelSearch"}},{"displayName":"ID","name":"id","type":"string"}],"routing":{"send":{"type":"body","property":"model","value":"={{$parameter.model.value}}"}}},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Base URL","name":"baseURL","default":"https://api.openai.com/v1","description":"Override the default base URL for the API","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":-1,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Timeout","name":"timeout","default":60000,"description":"Maximum amount of time a request is allowed to take in milliseconds","type":"number"},{"displayName":"Max Retries","name":"maxRetries","default":2,"description":"Maximum number of retries to attempt","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":{"light":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOpenAi/openAiLight.svg","dark":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOpenAi/openAiLight.dark.svg"}},
{"displayName":"Cohere Model","name":"@n8n/n8n-nodes-langchain.lmCohere","group":["transform"],"version":1,"description":"Language Model Cohere","defaults":{"name":"Cohere Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmcohere/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"cohereApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":250,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Model","name":"model","type":"string","description":"The name of the model to use","default":""},{"displayName":"Sampling Temperature","name":"temperature","default":0,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMCohere/cohere.svg"},
{"displayName":"Ollama Model","name":"@n8n/n8n-nodes-langchain.lmOllama","group":["transform"],"version":1,"description":"Language Model Ollama","defaults":{"name":"Ollama Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"ollamaApi","required":true}],"requestDefaults":{"ignoreHttpStatusErrors":true,"baseURL":"={{ $credentials.baseUrl.replace(new RegExp(\"/$\"), \"\") }}"},"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"options","default":"llama2","description":"The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.","typeOptions":{"loadOptions":{"routing":{"request":{"method":"GET","url":"/api/tags"},"output":{"postReceive":[{"type":"rootProperty","properties":{"property":"models"}},{"type":"setKeyValue","properties":{"name":"={{$responseItem.name}}","value":"={{$responseItem.name}}"}},{"type":"sort","properties":{"key":"name"}}]}}}},"routing":{"send":{"type":"body","property":"model"}},"required":true},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Sampling Temperature","name":"temperature","default":0.7,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.","type":"number"},{"displayName":"Top K","name":"topK","default":-1,"typeOptions":{"maxValue":100,"minValue":-1,"numberPrecision":1},"description":"Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.","type":"number"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","type":"number","default":0,"typeOptions":{"minValue":0},"description":"Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition."},{"displayName":"Keep Alive","name":"keepAlive","type":"string","default":"5m","description":"Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes)."},{"displayName":"Low VRAM Mode","name":"lowVram","type":"boolean","default":false,"description":"Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory."},{"displayName":"Main GPU ID","name":"mainGpu","type":"number","default":0,"description":"Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs."},{"displayName":"Context Batch Size","name":"numBatch","type":"number","default":512,"description":"Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage."},{"displayName":"Context Length","name":"numCtx","type":"number","default":2048,"description":"The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model."},{"displayName":"Number of GPUs","name":"numGpu","type":"number","default":-1,"description":"Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection."},{"displayName":"Max Tokens to Generate","name":"numPredict","type":"number","default":-1,"description":"The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs."},{"displayName":"Number of CPU Threads","name":"numThread","type":"number","default":0,"description":"Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection."},{"displayName":"Penalize Newlines","name":"penalizeNewline","type":"boolean","default":true,"description":"Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text"},{"displayName":"Presence Penalty","name":"presencePenalty","type":"number","default":0,"description":"Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity."},{"displayName":"Repetition Penalty","name":"repeatPenalty","type":"number","default":1,"description":"Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty."},{"displayName":"Use Memory Locking","name":"useMLock","type":"boolean","default":false,"description":"Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory."},{"displayName":"Use Memory Mapping","name":"useMMap","type":"boolean","default":true,"description":"Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled."},{"displayName":"Load Vocabulary Only","name":"vocabOnly","type":"boolean","default":false,"description":"Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization."},{"displayName":"Output Format","name":"format","type":"options","options":[{"name":"Default","value":"default"},{"name":"JSON","value":"json"}],"default":"default","description":"Specifies the format of the API response"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOllama/ollama.svg"},
{"displayName":"Hugging Face Inference Model","name":"@n8n/n8n-nodes-langchain.lmOpenHuggingFaceInference","group":["transform"],"version":1,"description":"Language Model HuggingFaceInference","defaults":{"name":"Hugging Face Inference Model"},"codex":{"categories":["AI"],"subcategories":{"AI":["Language Models"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmopenhuggingfaceinference/"}]}},"inputs":[],"outputs":["ai_languageModel"],"outputNames":["Model"],"credentials":[{"name":"huggingFaceApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Model","name":"model","type":"string","default":"gpt2"},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Custom Inference Endpoint","name":"endpointUrl","default":"","description":"Custom endpoint URL","type":"string"},{"displayName":"Frequency Penalty","name":"frequencyPenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim","type":"number"},{"displayName":"Maximum Number of Tokens","name":"maxTokens","default":128,"description":"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).","type":"number","typeOptions":{"maxValue":32768}},{"displayName":"Presence Penalty","name":"presencePenalty","default":0,"typeOptions":{"maxValue":2,"minValue":-2,"numberPrecision":1},"description":"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics","type":"number"},{"displayName":"Sampling Temperature","name":"temperature","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.","type":"number"},{"displayName":"Top K","name":"topK","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls the top tokens to consider within the sample operation to create new text","type":"number"},{"displayName":"Top P","name":"topP","default":1,"typeOptions":{"maxValue":1,"minValue":0,"numberPrecision":1},"description":"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/llms/LMOpenHuggingFaceInference/huggingface.svg"},
{"displayName":"Window Buffer Memory (easiest)","name":"@n8n/n8n-nodes-langchain.memoryBufferWindow","icon":"fa:database","group":["transform"],"version":[1,1.1,1.2],"description":"Stores in n8n memory, so no credentials required","defaults":{"name":"Window Buffer Memory"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Session Key","name":"sessionKey","type":"string","default":"chat_history","description":"The key to use to store the memory in the workflow data","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Session ID","name":"sessionKey","type":"string","default":"={{ $json.sessionId }}","description":"The key to use to store the memory","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Session ID","name":"sessionIdType","type":"options","options":[{"name":"Take from previous node automatically","value":"fromInput","description":"Looks for an input field called sessionId"},{"name":"Define below","value":"customKey","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"fromInput","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"Key","name":"sessionKey","type":"string","default":"","description":"The key to use to store session ID in the memory","displayOptions":{"show":{"sessionIdType":["customKey"]}}},{"displayName":"Context Window Length","name":"contextWindowLength","type":"number","default":5,"description":"The number of previous messages to consider for context"},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Motorhead","name":"@n8n/n8n-nodes-langchain.memoryMotorhead","icon":"fa:file-export","group":["transform"],"version":[1,1.1,1.2],"description":"Use Motorhead Memory","defaults":{"name":"Motorhead"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymotorhead/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"motorheadApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":"","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Session ID","name":"sessionId","type":"string","default":"={{ $json.sessionId }}","description":"The key to use to store the memory","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Session ID","name":"sessionIdType","type":"options","options":[{"name":"Take from previous node automatically","value":"fromInput","description":"Looks for an input field called sessionId"},{"name":"Define below","value":"customKey","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"fromInput","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"Key","name":"sessionKey","type":"string","default":"","description":"The key to use to store session ID in the memory","displayOptions":{"show":{"sessionIdType":["customKey"]}}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Redis Chat Memory","name":"@n8n/n8n-nodes-langchain.memoryRedisChat","group":["transform"],"version":[1,1.1,1.2],"description":"Stores the chat history in Redis.","defaults":{"name":"Redis Chat Memory"},"credentials":[{"name":"redis","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryredischat/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Session Key","name":"sessionKey","type":"string","default":"chat_history","description":"The key to use to store the memory in the workflow data","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Session ID","name":"sessionKey","type":"string","default":"={{ $json.sessionId }}","description":"The key to use to store the memory","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Session ID","name":"sessionIdType","type":"options","options":[{"name":"Take from previous node automatically","value":"fromInput","description":"Looks for an input field called sessionId"},{"name":"Define below","value":"customKey","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"fromInput","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"Key","name":"sessionKey","type":"string","default":"","description":"The key to use to store session ID in the memory","displayOptions":{"show":{"sessionIdType":["customKey"]}}},{"displayName":"Session Time To Live","name":"sessionTTL","type":"number","default":0,"description":"For how long the session should be stored in seconds. If set to 0 it will not expire."},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryRedisChat/redis.svg"},
{"displayName":"Chat Memory Manager","name":"@n8n/n8n-nodes-langchain.memoryManager","icon":"fa:database","iconColor":"black","group":["transform"],"version":[1,1.1],"description":"Manage chat messages memory and use it in the workflow","defaults":{"name":"Chat Memory Manager"},"codex":{"categories":["AI"],"subcategories":{"AI":["Miscellaneous","Root Nodes"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/"}]}},"inputs":[{"displayName":"","type":"main"},{"displayName":"Memory","type":"ai_memory","required":true,"maxConnections":1}],"outputs":[{"displayName":"","type":"main"}],"properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"load","options":[{"name":"Get Many Messages","description":"Retrieve chat messages from connected memory","value":"load"},{"name":"Insert Messages","description":"Insert chat messages into connected memory","value":"insert"},{"name":"Delete Messages","description":"Delete chat messages from connected memory","value":"delete"}]},{"displayName":"Insert Mode","name":"insertMode","type":"options","description":"Choose how new messages are inserted into the memory","noDataExpression":true,"default":"insert","options":[{"name":"Insert Messages","value":"insert","description":"Add messages alongside existing ones"},{"name":"Override All Messages","value":"override","description":"Replace the current memory with new messages"}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Delete Mode","name":"deleteMode","type":"options","description":"How messages are deleted from memory","noDataExpression":true,"default":"lastN","options":[{"name":"Last N","value":"lastN","description":"Delete the last N messages"},{"name":"All Messages","value":"all","description":"Clear all messages from memory"}],"displayOptions":{"show":{"mode":["delete"]}}},{"displayName":"Chat Messages","name":"messages","description":"Chat messages to insert into memory","type":"fixedCollection","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add message","options":[{"name":"messageValues","displayName":"Message","values":[{"displayName":"Type Name or ID","name":"type","type":"options","options":[{"name":"AI","value":"ai"},{"name":"System","value":"system"},{"name":"User","value":"user"}],"default":"system"},{"displayName":"Message","name":"message","type":"string","required":true,"default":""},{"displayName":"Hide Message in Chat","name":"hideFromUI","type":"boolean","required":true,"default":false,"description":"Whether to hide the message from the chat UI"}]}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Messages Count","name":"lastMessagesCount","type":"number","description":"The amount of last messages to delete","default":2,"displayOptions":{"show":{"mode":["delete"],"deleteMode":["lastN"]}}},{"displayName":"Simplify Output","name":"simplifyOutput","type":"boolean","description":"Whether to simplify the output to only include the sender and the text","default":true,"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","placeholder":"Add Option","type":"collection","default":{},"options":[{"displayName":"Group Messages","name":"groupMessages","type":"boolean","default":true,"description":"Whether to group messages into a single item or return each message as a separate item"}],"displayOptions":{"show":{"mode":["load"]}}}]},
{"displayName":"Chat Messages Retriever","name":"@n8n/n8n-nodes-langchain.memoryChatRetriever","icon":"fa:database","group":["transform"],"hidden":true,"version":1,"description":"Retrieve chat messages from memory and use them in the workflow","defaults":{"name":"Chat Messages Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Miscellaneous"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorymanager/"}]}},"inputs":["main",{"displayName":"Memory","maxConnections":1,"type":"ai_memory","required":true}],"outputs":["main"],"properties":[{"displayName":"This node is deprecated. Use 'Chat Memory Manager' node instead.","type":"notice","default":"","name":"deprecatedNotice"},{"displayName":"Simplify Output","name":"simplifyOutput","type":"boolean","description":"Whether to simplify the output to only include the sender and the text","default":true}]},
{"displayName":"Xata","name":"@n8n/n8n-nodes-langchain.memoryXata","group":["transform"],"version":[1,1.1,1.2],"description":"Use Xata Memory","defaults":{"name":"Xata","color":"#1321A7"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryxata/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"xataApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":"","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Session ID","name":"sessionId","type":"string","default":"={{ $json.sessionId }}","description":"The key to use to store the memory","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Session ID","name":"sessionIdType","type":"options","options":[{"name":"Take from previous node automatically","value":"fromInput","description":"Looks for an input field called sessionId"},{"name":"Define below","value":"customKey","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"fromInput","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"Key","name":"sessionKey","type":"string","default":"","description":"The key to use to store session ID in the memory","displayOptions":{"show":{"sessionIdType":["customKey"]}}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryXata/xata.svg"},
{"displayName":"Zep","name":"@n8n/n8n-nodes-langchain.memoryZep","group":["transform"],"version":[1,1.1,1.2],"description":"Use Zep Memory","defaults":{"name":"Zep"},"codex":{"categories":["AI"],"subcategories":{"AI":["Memory"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memoryzep/"}]}},"inputs":[],"outputs":["ai_memory"],"outputNames":["Memory"],"credentials":[{"name":"zepApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Session ID","name":"sessionId","type":"string","required":true,"default":"","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Session ID","name":"sessionId","type":"string","default":"={{ $json.sessionId }}","description":"The key to use to store the memory","displayOptions":{"show":{"@version":[1.1]}}},{"displayName":"Session ID","name":"sessionIdType","type":"options","options":[{"name":"Take from previous node automatically","value":"fromInput","description":"Looks for an input field called sessionId"},{"name":"Define below","value":"customKey","description":"Use an expression to reference data in previous nodes or enter static text"}],"default":"fromInput","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"Key","name":"sessionKey","type":"string","default":"","description":"The key to use to store session ID in the memory","displayOptions":{"show":{"sessionIdType":["customKey"]}}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/memory/MemoryZep/zep.png"},
{"displayName":"Auto-fixing Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserAutofixing","icon":"fa:tools","group":["transform"],"version":1,"description":"Automatically fix the output if it is not in the correct format","defaults":{"name":"Auto-fixing Output Parser"},"codex":{"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserautofixing/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Output Parser","maxConnections":1,"required":true,"type":"ai_outputParser"}],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[{"displayName":"This node wraps another output parser. If the first one fails it calls an LLM to fix the format","name":"info","type":"notice","default":""},{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Item List Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserItemList","icon":"fa:bars","group":["transform"],"version":1,"description":"Return the results as separate items","defaults":{"name":"Item List Output Parser"},"codex":{"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparseritemlist/"}]}},"inputs":[],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Number Of Items","name":"numberOfItems","type":"number","default":-1,"description":"Defines how many items should be returned maximally. If set to -1, there is no limit."},{"displayName":"Separator","name":"separator","type":"string","default":"\\n","description":"Defines the separator that should be used to split the results into separate items. Defaults to a new line but can be changed depending on the data that should be returned."}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Structured Output Parser","name":"@n8n/n8n-nodes-langchain.outputParserStructured","icon":"fa:code","group":["transform"],"version":[1,1.1,1.2],"defaultVersion":1.2,"description":"Return data in a defined JSON format","defaults":{"name":"Structured Output Parser"},"codex":{"alias":["json","zod"],"categories":["AI"],"subcategories":{"AI":["Output Parsers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/"}]}},"inputs":[],"outputs":["ai_outputParser"],"outputNames":["Output Parser"],"properties":[{"displayName":"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Schema Type","name":"schemaType","type":"options","noDataExpression":true,"options":[{"name":"Generate From JSON Example","value":"fromJson","description":"Generate a schema from an example JSON object"},{"name":"Define Below","value":"manual","description":"Define the JSON schema manually"}],"default":"fromJson","description":"How to specify the schema for the function","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"JSON Example","name":"jsonSchemaExample","type":"json","default":"{\n\t\"state\": \"California\",\n\t\"cities\": [\"Los Angeles\", \"San Francisco\", \"San Diego\"]\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["fromJson"]}},"description":"Example JSON object to use to generate the schema"},{"displayName":"JSON Schema","name":"inputSchema","type":"json","default":"{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"state\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"cities\": {\n\t\t\t\"type\": \"array\",\n\t\t\t\"items\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t}\n\t\t}\n\t}\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["manual"]}},"description":"JSON Schema to structure and validate the output against"},{"displayName":"Schema Type","name":"schemaType","type":"options","noDataExpression":true,"options":[{"name":"Generate From JSON Example","value":"fromJson","description":"Generate a schema from an example JSON object"},{"name":"Define Below","value":"manual","description":"Define the JSON schema manually"}],"default":"fromJson","description":"How to specify the schema for the function","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.2}}]}}},{"displayName":"JSON Example","name":"jsonSchemaExample","type":"json","default":"{\n\t\"state\": \"California\",\n\t\"cities\": [\"Los Angeles\", \"San Francisco\", \"San Diego\"]\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["fromJson"]}},"description":"Example JSON object to use to generate the schema"},{"displayName":"Input Schema","name":"inputSchema","type":"json","default":"{\n\t\"type\": \"object\",\n\t\"properties\": {\n\t\t\"state\": {\n\t\t\t\"type\": \"string\"\n\t\t},\n\t\t\"cities\": {\n\t\t\t\"type\": \"array\",\n\t\t\t\"items\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t}\n\t\t}\n\t}\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["manual"]}},"description":"Schema to use for the function"},{"displayName":"JSON Schema","name":"jsonSchema","type":"json","description":"JSON Schema to structure and validate the output against","default":"{\n  \"type\": \"object\",\n  \"properties\": {\n    \"state\": {\n      \"type\": \"string\"\n    },\n    \"cities\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      }\n    }\n  }\n}","typeOptions":{"rows":10},"required":true,"displayOptions":{"show":{"@version":[{"_cnd":{"lte":1.1}}]}}},{"displayName":"The schema has to be defined in the <a target=\"_blank\" href=\"https://json-schema.org/\">JSON Schema</a> format. Look at <a target=\"_blank\" href=\"https://json-schema.org/learn/miscellaneous-examples.html\">this</a> page for examples.","name":"notice","type":"notice","default":"","displayOptions":{"hide":{"schemaType":["fromJson"]}}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Contextual Compression Retriever","name":"@n8n/n8n-nodes-langchain.retrieverContextualCompression","icon":"fa:box-open","group":["transform"],"version":1,"description":"Enhances document similarity search by contextual compression.","defaults":{"name":"Contextual Compression Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievercontextualcompression/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Vector Store Retriever","name":"@n8n/n8n-nodes-langchain.retrieverVectorStore","icon":"fa:box-open","group":["transform"],"version":1,"description":"Use a Vector Store as Retriever","defaults":{"name":"Vector Store Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore/"}]}},"inputs":[{"displayName":"Vector Store","maxConnections":1,"type":"ai_vectorStore","required":true}],"outputs":["ai_retriever"],"outputNames":["Retriever"],"properties":[{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"The maximum number of results to return"},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"MultiQuery Retriever","name":"@n8n/n8n-nodes-langchain.retrieverMultiQuery","icon":"fa:box-open","group":["transform"],"version":1,"description":"Automates prompt tuning, generates diverse queries and expands document pool for enhanced retrieval.","defaults":{"name":"MultiQuery Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievermultiquery/"}]}},"inputs":[{"displayName":"Model","maxConnections":1,"type":"ai_languageModel","required":true},{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever","required":true}],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Query Count","name":"queryCount","default":3,"typeOptions":{"minValue":1},"description":"Number of different versions of the given question to generate","type":"number"}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Workflow Retriever","name":"@n8n/n8n-nodes-langchain.retrieverWorkflow","icon":"fa:box-open","group":["transform"],"version":1,"description":"Use an n8n Workflow as Retriever","defaults":{"name":"Workflow Retriever"},"codex":{"categories":["AI"],"subcategories":{"AI":["Retrievers"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrieverworkflow/"}]}},"inputs":[],"outputs":[{"displayName":"Retriever","maxConnections":1,"type":"ai_retriever"}],"properties":[{"displayName":"The workflow will receive \"query\" as input and the output of the last node will be returned and converted to Documents","name":"executeNotice","type":"notice","default":""},{"displayName":"Source","name":"source","type":"options","options":[{"name":"Database","value":"database","description":"Load the workflow from the database by ID"},{"name":"Parameter","value":"parameter","description":"Load the workflow from a parameter"}],"default":"database","description":"Where to get the workflow to execute from"},{"displayName":"Workflow ID","name":"workflowId","type":"string","displayOptions":{"show":{"source":["database"]}},"default":"","required":true,"description":"The workflow to execute"},{"displayName":"Workflow JSON","name":"workflowJson","type":"json","typeOptions":{"rows":10},"displayOptions":{"show":{"source":["parameter"]}},"default":"\n\n\n","required":true,"description":"The workflow JSON code to execute"},{"displayName":"Workflow Values","name":"fields","placeholder":"Add Value","type":"fixedCollection","description":"Set the values which should be made available in the workflow","typeOptions":{"multipleValues":true,"sortable":true},"default":{},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. fieldName","description":"Name of the field to set the value of. Supports dot-notation. Example: data.person[0].name.","requiresDataPath":"single"},{"displayName":"Type","name":"type","type":"options","description":"The field value type","options":[{"name":"String","value":"stringValue"},{"name":"Number","value":"numberValue"},{"name":"Boolean","value":"booleanValue"},{"name":"Array","value":"arrayValue"},{"name":"Object","value":"objectValue"}],"default":"stringValue"},{"displayName":"Value","name":"stringValue","type":"string","default":"","displayOptions":{"show":{"type":["stringValue"]}},"validateType":"string","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"numberValue","type":"string","default":"","displayOptions":{"show":{"type":["numberValue"]}},"validateType":"number","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"booleanValue","type":"options","default":"true","options":[{"name":"True","value":"true"},{"name":"False","value":"false"}],"displayOptions":{"show":{"type":["booleanValue"]}},"validateType":"boolean","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"arrayValue","type":"string","default":"","placeholder":"e.g. [ arrayItem1, arrayItem2, arrayItem3 ]","displayOptions":{"show":{"type":["arrayValue"]}},"validateType":"array","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"objectValue","type":"json","default":"={}","typeOptions":{"rows":2},"displayOptions":{"show":{"type":["objectValue"]}},"validateType":"object","ignoreValidationDuringExecution":true}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Character Text Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterCharacterTextSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by characters","defaults":{"name":"Character Text Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittercharactertextsplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Separator","name":"separator","type":"string","default":""},{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Recursive Character Text Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterRecursiveCharacterTextSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by characters recursively, recommended for most use cases","defaults":{"name":"Recursive Character Text Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0},{"displayName":"Options","name":"options","placeholder":"Add Option","description":"Additional options to add","type":"collection","default":{},"options":[{"displayName":"Split Code","name":"splitCode","default":"markdown","type":"options","options":[{"name":"cpp","value":"cpp"},{"name":"go","value":"go"},{"name":"java","value":"java"},{"name":"js","value":"js"},{"name":"php","value":"php"},{"name":"proto","value":"proto"},{"name":"python","value":"python"},{"name":"rst","value":"rst"},{"name":"ruby","value":"ruby"},{"name":"rust","value":"rust"},{"name":"scala","value":"scala"},{"name":"swift","value":"swift"},{"name":"markdown","value":"markdown"},{"name":"latex","value":"latex"},{"name":"html","value":"html"}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Token Splitter","name":"@n8n/n8n-nodes-langchain.textSplitterTokenSplitter","icon":"fa:grip-lines-vertical","group":["transform"],"version":1,"description":"Split text into chunks by tokens","defaults":{"name":"Token Splitter"},"codex":{"categories":["AI"],"subcategories":{"AI":["Text Splitters"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplittertokensplitter/"}]}},"inputs":[],"outputs":["ai_textSplitter"],"outputNames":["Text Splitter"],"properties":[{"displayName":"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Chunk Size","name":"chunkSize","type":"number","default":1000},{"displayName":"Chunk Overlap","name":"chunkOverlap","type":"number","default":0},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Calculator","name":"@n8n/n8n-nodes-langchain.toolCalculator","icon":"fa:calculator","group":["transform"],"version":1,"description":"Make it easier for AI agents to perform arithmetic","defaults":{"name":"Calculator"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcalculator/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Custom Code Tool","name":"@n8n/n8n-nodes-langchain.toolCode","icon":"fa:code","group":["transform"],"version":[1,1.1],"description":"Write a tool in JS or Python","defaults":{"name":"Custom Code Tool"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolcode/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"See an example of a conversational agent with custom tool written in JavaScript <a href=\"/templates/1963\" target=\"_blank\">here</a>.","name":"noticeTemplateExample","type":"notice","default":""},{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"My_Tool","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. My_Tool","validateType":"string-alphanumeric","description":"The name of the function to be called, could contain letters, numbers, and underscores only","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.1}}]}}},{"displayName":"Description","name":"description","type":"string","default":"","placeholder":"Call this tool to get a random color. The input should be a string with comma separted names of colors to exclude.","typeOptions":{"rows":3}},{"displayName":"Language","name":"language","type":"options","noDataExpression":true,"options":[{"name":"JavaScript","value":"javaScript"},{"name":"Python (Beta)","value":"python"}],"default":"javaScript"},{"displayName":"JavaScript","name":"jsCode","type":"string","displayOptions":{"show":{"language":["javaScript"]}},"typeOptions":{"editor":"jsEditor"},"default":"// Example: convert the incoming query to uppercase and return it\nreturn query.toUpperCase()","hint":"You can access the input the tool receives via the input property \"query\". The returned value should be a single string.","description":"E.g. Converts any text to uppercase","noDataExpression":true},{"displayName":"Python","name":"pythonCode","type":"string","displayOptions":{"show":{"language":["python"]}},"typeOptions":{"editor":"codeNodeEditor","editorLanguage":"python"},"default":"# Example: convert the incoming query to uppercase and return it\nreturn query.upper()","hint":"You can access the input the tool receives via the input property \"query\". The returned value should be a single string.","description":"E.g. Converts any text to uppercase","noDataExpression":true},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"HTTP Request Tool","name":"@n8n/n8n-nodes-langchain.toolHttpRequest","group":["output"],"version":1,"description":"Makes an HTTP request and returns the response data","subtitle":"={{ $parameter.toolDescription }}","defaults":{"name":"HTTP Request"},"credentials":[],"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolhttprequest/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Description","name":"toolDescription","type":"string","description":"Explain to LLM what this tool does, better description would allow LLM to produce expected result","placeholder":"e.g. Get the current weather in the requested city","default":"","typeOptions":{"rows":3}},{"displayName":"Method","name":"method","type":"options","options":[{"name":"DELETE","value":"DELETE"},{"name":"GET","value":"GET"},{"name":"PATCH","value":"PATCH"},{"name":"POST","value":"POST"},{"name":"PUT","value":"PUT"}],"default":"GET"},{"displayName":"Tip: You can use a {placeholder} for any part of the request to be filled by the model. Provide more context about them in the placeholders section","name":"placeholderNotice","type":"notice","default":""},{"displayName":"URL","name":"url","type":"string","default":"","required":true,"placeholder":"e.g. http://www.example.com/{path}"},{"displayName":"Authentication","name":"authentication","description":"Select the type of authentication to use if needed, authentication would be done by n8n and your credentials will not be shared with the LLM","noDataExpression":true,"type":"options","options":[{"name":"None","value":"none"},{"name":"Predefined Credential Type","value":"predefinedCredentialType","description":"We've already implemented auth for many services so that you don't have to set it up manually"},{"name":"Generic Credential Type","value":"genericCredentialType","description":"Fully customizable. Choose between basic, header, OAuth2, etc."}],"default":"none"},{"displayName":"Credential Type","name":"nodeCredentialType","type":"credentialsSelect","noDataExpression":true,"required":true,"default":"","credentialTypes":["extends:oAuth2Api","extends:oAuth1Api","has:authenticate"],"displayOptions":{"show":{"authentication":["predefinedCredentialType"]}}},{"displayName":"Make sure you have specified the scope(s) for the Service Account in the credential","name":"googleApiWarning","type":"notice","default":"","displayOptions":{"show":{"nodeCredentialType":["googleApi"]}}},{"displayName":"Generic Auth Type","name":"genericAuthType","type":"credentialsSelect","required":true,"default":"","credentialTypes":["has:genericAuth"],"displayOptions":{"show":{"authentication":["genericCredentialType"]}}},{"displayName":"Send Query Parameters","name":"sendQuery","type":"boolean","default":false,"noDataExpression":true,"description":"Whether the request has query params or not"},{"displayName":"Specify Query Parameters","name":"specifyQuery","type":"options","options":[{"name":"Using Fields Below","value":"keypair"},{"name":"Using JSON Below","value":"json"},{"name":"Let Model Specify Entire Body","value":"model"}],"default":"keypair","displayOptions":{"show":{"sendQuery":[true]}}},{"displayName":"Query Parameters","name":"parametersQuery","type":"fixedCollection","typeOptions":{"multipleValues":true},"placeholder":"Add Parameter","default":{"values":[{"name":""}]},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":""},{"displayName":"Value Provided","name":"valueProvider","type":"options","options":[{"name":"By Model (and is required)","value":"modelRequired"},{"name":"By Model (but is optional)","value":"modelOptional"},{"name":"Using Field Below","value":"fieldValue"}],"default":"modelRequired"},{"displayName":"Value","name":"value","type":"string","default":"","hint":"Use a {placeholder} for any data to be filled in by the model","displayOptions":{"show":{"valueProvider":["fieldValue"]}}}]}],"displayOptions":{"show":{"sendQuery":[true],"specifyQuery":["keypair"]}}},{"displayName":"JSON","name":"jsonQuery","type":"string","typeOptions":{"rows":5},"hint":"Use a {placeholder} for any data to be filled in by the model","default":"","displayOptions":{"show":{"sendQuery":[true],"specifyQuery":["json"]}}},{"displayName":"Send Headers","name":"sendHeaders","type":"boolean","default":false,"noDataExpression":true,"description":"Whether the request has headers or not"},{"displayName":"Specify Headers","name":"specifyHeaders","type":"options","options":[{"name":"Using Fields Below","value":"keypair"},{"name":"Using JSON Below","value":"json"},{"name":"Let Model Specify Entire Body","value":"model"}],"default":"keypair","displayOptions":{"show":{"sendHeaders":[true]}}},{"displayName":"Header Parameters","name":"parametersHeaders","type":"fixedCollection","typeOptions":{"multipleValues":true},"placeholder":"Add Parameter","default":{"values":[{"name":""}]},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":""},{"displayName":"Value Provided","name":"valueProvider","type":"options","options":[{"name":"By Model (and is required)","value":"modelRequired"},{"name":"By Model (but is optional)","value":"modelOptional"},{"name":"Using Field Below","value":"fieldValue"}],"default":"modelRequired"},{"displayName":"Value","name":"value","type":"string","default":"","hint":"Use a {placeholder} for any data to be filled in by the model","displayOptions":{"show":{"valueProvider":["fieldValue"]}}}]}],"displayOptions":{"show":{"sendHeaders":[true],"specifyHeaders":["keypair"]}}},{"displayName":"JSON","name":"jsonHeaders","type":"string","typeOptions":{"rows":5},"hint":"Use a {placeholder} for any data to be filled in by the model","default":"","displayOptions":{"show":{"sendHeaders":[true],"specifyHeaders":["json"]}}},{"displayName":"Send Body","name":"sendBody","type":"boolean","default":false,"noDataExpression":true,"description":"Whether the request has body or not"},{"displayName":"Specify Body","name":"specifyBody","type":"options","options":[{"name":"Using Fields Below","value":"keypair"},{"name":"Using JSON Below","value":"json"},{"name":"Let Model Specify Entire Body","value":"model"}],"default":"keypair","displayOptions":{"show":{"sendBody":[true]}}},{"displayName":"Body Parameters","name":"parametersBody","type":"fixedCollection","typeOptions":{"multipleValues":true},"placeholder":"Add Parameter","default":{"values":[{"name":""}]},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":""},{"displayName":"Value Provided","name":"valueProvider","type":"options","options":[{"name":"By Model (and is required)","value":"modelRequired"},{"name":"By Model (but is optional)","value":"modelOptional"},{"name":"Using Field Below","value":"fieldValue"}],"default":"modelRequired"},{"displayName":"Value","name":"value","type":"string","default":"","hint":"Use a {placeholder} for any data to be filled in by the model","displayOptions":{"show":{"valueProvider":["fieldValue"]}}}]}],"displayOptions":{"show":{"sendBody":[true],"specifyBody":["keypair"]}}},{"displayName":"JSON","name":"jsonBody","type":"string","typeOptions":{"rows":5},"hint":"Use a {placeholder} for any data to be filled in by the model","default":"","displayOptions":{"show":{"sendBody":[true],"specifyBody":["json"]}}},{"displayName":"Placeholder Definitions","name":"placeholderDefinitions","type":"fixedCollection","typeOptions":{"multipleValues":true},"placeholder":"Add Definition","default":[],"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Placeholder Name","name":"name","type":"string","default":""},{"displayName":"Description","name":"description","type":"string","default":""},{"displayName":"Type","name":"type","type":"options","options":[{"name":"Not Specified (Default)","value":"not specified"},{"name":"String","value":"string"},{"name":"Number","value":"number"},{"name":"Boolean","value":"boolean"},{"name":"JSON","value":"json"}],"default":"not specified"}]}]},{"displayName":"Optimize Response","name":"optimizeResponse","type":"boolean","default":false,"noDataExpression":true,"description":"Whether the optimize the tool response to reduce amount of data passed to the LLM that could lead to better result and reduce cost"},{"displayName":"Expected Response Type","name":"responseType","type":"options","displayOptions":{"show":{"optimizeResponse":[true]}},"options":[{"name":"JSON","value":"json"},{"name":"HTML","value":"html"},{"name":"Text","value":"text"}],"default":"json"},{"displayName":"Field Containing Data","name":"dataField","type":"string","default":"","placeholder":"e.g. records","description":"Specify the name of the field in the response containing the data","hint":"leave blank to use whole response","requiresDataPath":"single","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["json"]}}},{"displayName":"Include Fields","name":"fieldsToInclude","type":"options","description":"What fields response object should include","default":"all","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["json"]}},"options":[{"name":"All","value":"all","description":"Include all fields"},{"name":"Selected","value":"selected","description":"Include only fields specified below"},{"name":"Except","value":"except","description":"Exclude fields specified below"}]},{"displayName":"Fields","name":"fields","type":"string","default":"","placeholder":"e.g. field1,field2","description":"Comma-separated list of the field names. Supports dot notation. You can drag the selected fields from the input panel.","requiresDataPath":"multiple","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["json"]},"hide":{"fieldsToInclude":["all"]}}},{"displayName":"Selector (CSS)","name":"cssSelector","type":"string","description":"Select specific element(e.g. body) or multiple elements(e.g. div) of chosen type in the response HTML.","placeholder":"e.g. body","default":"body","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["html"]}}},{"displayName":"Return Only Content","name":"onlyContent","type":"boolean","default":false,"description":"Whether to return only content of html elements, stripping html tags and attributes","hint":"Uses less tokens and may be easier for model to understand","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["html"]}}},{"displayName":"Elements To Omit","name":"elementsToOmit","type":"string","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["html"],"onlyContent":[true]}},"default":"","placeholder":"e.g. img, .className, #ItemId","description":"Comma-separated list of selectors that would be excluded when extracting content"},{"displayName":"Truncate Response","name":"truncateResponse","type":"boolean","default":false,"hint":"Helps save tokens","displayOptions":{"show":{"optimizeResponse":[true],"responseType":["text","html"]}}},{"displayName":"Max Response Characters","name":"maxLength","type":"number","default":1000,"typeOptions":{"minValue":1},"displayOptions":{"show":{"optimizeResponse":[true],"responseType":["text","html"],"truncateResponse":[true]}}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":{"light":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolHttpRequest/httprequest.svg","dark":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolHttpRequest/httprequest.dark.svg"}},
{"displayName":"SerpApi (Google Search)","name":"@n8n/n8n-nodes-langchain.toolSerpApi","group":["transform"],"version":1,"description":"Search in Google using SerpAPI","defaults":{"name":"SerpAPI"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolserpapi/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"credentials":[{"name":"serpApi","required":true}],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Country","name":"gl","type":"string","default":"us","description":"Defines the country to use for search. Head to <a href=\"https://serpapi.com/google-countries\">Google countries page</a> for a full list of supported countries."},{"displayName":"Device","name":"device","type":"options","options":[{"name":"Desktop","value":"desktop"},{"name":"Mobile","value":"mobile"},{"name":"Tablet","value":"tablet"}],"default":"desktop","description":"Device to use to get the results"},{"displayName":"Explicit Array","name":"no_cache","type":"boolean","default":false,"description":"Whether to force SerpApi to fetch the Google results even if a cached version is already present. Cache expires after 1h. Cached searches are free, and are not counted towards your searches per month."},{"displayName":"Google Domain","name":"google_domain","type":"string","default":"google.com","description":"Defines the domain to use for search. Head to <a href=\"https://serpapi.com/google-domains\">Google domains page</a> for a full list of supported domains."},{"displayName":"Language","name":"hl","type":"string","default":"en","description":"Defines the language to use. It's a two-letter language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French). Head to <a href=\"https://serpapi.com/google-languages\">Google languages page</a> for a full list of supported languages."}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolSerpApi/serpApi.svg"},
{"displayName":"Wikipedia","name":"@n8n/n8n-nodes-langchain.toolWikipedia","group":["transform"],"version":1,"description":"Search in Wikipedia","defaults":{"name":"Wikipedia"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwikipedia/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolWikipedia/wikipedia.svg"},
{"displayName":"Wolfram|Alpha","name":"@n8n/n8n-nodes-langchain.toolWolframAlpha","group":["transform"],"version":1,"description":"Connects to WolframAlpha's computational intelligence engine.","defaults":{"name":"Wolfram Alpha"},"credentials":[{"name":"wolframAlphaApi","required":true}],"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolwolframalpha/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/tools/ToolWolframAlpha/wolfram-alpha.svg"},
{"displayName":"Custom n8n Workflow Tool","name":"@n8n/n8n-nodes-langchain.toolWorkflow","icon":"fa:network-wired","group":["transform"],"version":[1,1.1],"description":"Uses another n8n workflow as a tool. Allows packaging any n8n node(s) as a tool.","defaults":{"name":"Custom n8n Workflow Tool"},"codex":{"categories":["AI"],"subcategories":{"AI":["Tools"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolworkflow/"}]}},"inputs":[],"outputs":["ai_tool"],"outputNames":["Tool"],"properties":[{"displayName":"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"}},{"displayName":"See an example of a workflow to suggest meeting slots using AI <a href=\"/templates/1953\" target=\"_blank\">here</a>.","name":"noticeTemplateExample","type":"notice","default":""},{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"My_Color_Tool","displayOptions":{"show":{"@version":[1]}}},{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. My_Color_Tool","validateType":"string-alphanumeric","description":"The name of the function to be called, could contain letters, numbers, and underscores only","displayOptions":{"show":{"@version":[{"_cnd":{"gte":1.1}}]}}},{"displayName":"Description","name":"description","type":"string","default":"","placeholder":"Call this tool to get a random color. The input should be a string with comma separted names of colors to exclude.","typeOptions":{"rows":3}},{"displayName":"This tool will call the workflow you define below, and look in the last node for the response. The workflow needs to start with an Execute Workflow trigger","name":"executeNotice","type":"notice","default":""},{"displayName":"Source","name":"source","type":"options","options":[{"name":"Database","value":"database","description":"Load the workflow from the database by ID"},{"name":"Define Below","value":"parameter","description":"Pass the JSON code of a workflow"}],"default":"database","description":"Where to get the workflow to execute from"},{"displayName":"Workflow ID","name":"workflowId","type":"string","displayOptions":{"show":{"source":["database"]}},"default":"","required":true,"description":"The workflow to execute","hint":"Can be found in the URL of the workflow"},{"displayName":"Workflow JSON","name":"workflowJson","type":"json","typeOptions":{"rows":10},"displayOptions":{"show":{"source":["parameter"]}},"default":"\n\n\n\n\n\n\n\n\n","required":true,"description":"The workflow JSON code to execute"},{"displayName":"Field to Return","name":"responsePropertyName","type":"string","default":"response","required":true,"hint":"The field in the last-executed node of the workflow that contains the response","description":"Where to find the data that this tool should return. n8n will look in the output of the last-executed node of the workflow for a field with this name, and return its value."},{"displayName":"Extra Workflow Inputs","name":"fields","placeholder":"Add Value","type":"fixedCollection","description":"These will be output by the 'execute workflow' trigger of the workflow being called","typeOptions":{"multipleValues":true,"sortable":true},"default":{},"options":[{"name":"values","displayName":"Values","values":[{"displayName":"Name","name":"name","type":"string","default":"","placeholder":"e.g. fieldName","description":"Name of the field to set the value of. Supports dot-notation. Example: data.person[0].name.","requiresDataPath":"single"},{"displayName":"Type","name":"type","type":"options","description":"The field value type","options":[{"name":"String","value":"stringValue"},{"name":"Number","value":"numberValue"},{"name":"Boolean","value":"booleanValue"},{"name":"Array","value":"arrayValue"},{"name":"Object","value":"objectValue"}],"default":"stringValue"},{"displayName":"Value","name":"stringValue","type":"string","default":"","displayOptions":{"show":{"type":["stringValue"]}},"validateType":"string","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"numberValue","type":"string","default":"","displayOptions":{"show":{"type":["numberValue"]}},"validateType":"number","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"booleanValue","type":"options","default":"true","options":[{"name":"True","value":"true"},{"name":"False","value":"false"}],"displayOptions":{"show":{"type":["booleanValue"]}},"validateType":"boolean","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"arrayValue","type":"string","default":"","placeholder":"e.g. [ arrayItem1, arrayItem2, arrayItem3 ]","displayOptions":{"show":{"type":["arrayValue"]}},"validateType":"array","ignoreValidationDuringExecution":true},{"displayName":"Value","name":"objectValue","type":"json","default":"={}","typeOptions":{"rows":2},"displayOptions":{"show":{"type":["objectValue"]}},"validateType":"object","ignoreValidationDuringExecution":true}]}]},{"displayName":"Specify Input Schema","name":"specifyInputSchema","type":"boolean","description":"Whether to specify the schema for the function. This would require the LLM to provide the input in the correct format and would validate it against the schema.","noDataExpression":true,"default":false},{"displayName":"Schema Type","name":"schemaType","type":"options","noDataExpression":true,"options":[{"name":"Generate From JSON Example","value":"fromJson","description":"Generate a schema from an example JSON object"},{"name":"Define Below","value":"manual","description":"Define the JSON schema manually"}],"default":"fromJson","description":"How to specify the schema for the function","displayOptions":{"show":{"specifyInputSchema":[true]}}},{"displayName":"JSON Example","name":"jsonSchemaExample","type":"json","default":"{\n\t\"some_input\": \"some_value\"\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["fromJson"]}},"description":"Example JSON object to use to generate the schema"},{"displayName":"Input Schema","name":"inputSchema","type":"json","default":"{\n\"type\": \"object\",\n\"properties\": {\n\t\"some_input\": {\n\t\t\"type\": \"string\",\n\t\t\"description\": \"Some input to the function\"\n\t\t}\n\t}\n}","noDataExpression":true,"typeOptions":{"rows":10},"displayOptions":{"show":{"schemaType":["manual"]}},"description":"Schema to use for the function"},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Manual Chat Trigger","name":"@n8n/n8n-nodes-langchain.manualChatTrigger","icon":"fa:comments","group":["trigger"],"version":[1,1.1],"description":"Runs the flow on new manual chat message","eventTriggerDescription":"","maxNodes":1,"hidden":true,"defaults":{"name":"When chat message received","color":"#909298"},"codex":{"categories":["Core Nodes"],"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.chattrigger/"}]},"subcategories":{"Core Nodes":["Other Trigger Nodes"]}},"inputs":[],"outputs":["main"],"properties":[{"displayName":"This node is where a manual chat workflow execution starts. To make one, go back to the canvas and click Chat","name":"notice","type":"notice","default":""},{"displayName":"Chat and execute workflow","name":"openChat","type":"button","typeOptions":{"action":"openChat"},"default":""}]},
{"displayName":"Chat Trigger","name":"@n8n/n8n-nodes-langchain.chatTrigger","icon":"fa:comments","iconColor":"black","group":["trigger"],"version":1,"description":"Runs the workflow when an n8n generated webchat is submitted","defaults":{"name":"When chat message received"},"codex":{"categories":["Core Nodes"],"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.chattrigger/"}]}},"supportsCORS":true,"maxNodes":1,"inputs":"={{ (() => {\n\t\t\tif (!['hostedChat', 'webhook'].includes($parameter.mode)) {\n\t\t\t\treturn [];\n\t\t\t}\n\t\t\tif ($parameter.options?.loadPreviousSession !== 'memory') {\n\t\t\t\treturn [];\n\t\t\t}\n\n\t\t\treturn [\n\t\t\t\t{\n\t\t\t\t\tdisplayName: 'Memory',\n\t\t\t\t\tmaxConnections: 1,\n\t\t\t\t\ttype: 'ai_memory',\n\t\t\t\t\trequired: true,\n\t\t\t\t}\n\t\t\t];\n\t\t })() }}","outputs":["main"],"credentials":[{"name":"httpBasicAuth","required":true,"displayOptions":{"show":{"authentication":["basicAuth"]}}}],"webhooks":[{"name":"setup","httpMethod":"GET","responseMode":"onReceived","path":"chat","ndvHideUrl":true},{"name":"default","httpMethod":"POST","responseMode":"={{$parameter.options?.[\"responseMode\"] || \"lastNode\" }}","path":"chat","ndvHideMethod":true,"ndvHideUrl":"={{ !$parameter.public }}"}],"eventTriggerDescription":"Waiting for you to submit the chat","activationMessage":"You can now make calls to your production chat URL.","triggerPanel":false,"properties":[{"displayName":"Make Chat Publicly Available","name":"public","type":"boolean","default":false,"description":"Whether the chat should be publicly available or only accessible through the manual chat interface"},{"displayName":"Mode","name":"mode","type":"options","options":[{"name":"Hosted Chat","value":"hostedChat","description":"Chat on a page served by n8n"},{"name":"Embedded Chat","value":"webhook","description":"Chat through a widget embedded in another page, or by calling a webhook"}],"default":"hostedChat","displayOptions":{"show":{"public":[true]}}},{"displayName":"Chat will be live at the URL above once you activate this workflow. Live executions will show up in the executions tab","name":"hostedChatNotice","type":"notice","displayOptions":{"show":{"mode":["hostedChat"],"public":[true]}},"default":""},{"displayName":"Follow the instructions <a href=\"https://www.npmjs.com/package/@n8n/chat\" target=\"_blank\">here</a> to embed chat in a webpage (or just call the webhook URL at the top of this section). Chat will be live once you activate this workflow","name":"embeddedChatNotice","type":"notice","displayOptions":{"show":{"mode":["webhook"],"public":[true]}},"default":""},{"displayName":"Authentication","name":"authentication","type":"options","displayOptions":{"show":{"public":[true]}},"options":[{"name":"Basic Auth","value":"basicAuth","description":"Simple username and password (the same one for all users)"},{"name":"n8n User Auth","value":"n8nUserAuth","description":"Require user to be logged in with their n8n account"},{"name":"None","value":"none"}],"default":"none","description":"The way to authenticate"},{"displayName":"Initial Message(s)","name":"initialMessages","type":"string","displayOptions":{"show":{"mode":["hostedChat"],"public":[true]}},"typeOptions":{"rows":3},"default":"Hi there! \nMy name is Nathan. How can I assist you today?","description":"Default messages shown at the start of the chat, one per line"},{"displayName":"Options","name":"options","type":"collection","displayOptions":{"show":{"mode":["hostedChat","webhook"],"public":[true]}},"placeholder":"Add Field","default":{},"options":[{"displayName":"Allowed Origins (CORS)","name":"allowedOrigins","type":"string","default":"*","description":"Comma-separated list of URLs allowed for cross-origin non-preflight requests. Use * (default) to allow all origins."},{"displayName":"Input Placeholder","name":"inputPlaceholder","type":"string","displayOptions":{"show":{"/mode":["hostedChat"]}},"default":"Type your question..","placeholder":"e.g. Type your message here","description":"Shown as placeholder text in the chat input field"},{"displayName":"Load Previous Session","name":"loadPreviousSession","type":"options","options":[{"name":"Off","value":"notSupported","description":"Loading messages of previous session is turned off"},{"name":"From Memory","value":"memory","description":"Load session messages from memory"},{"name":"Manually","value":"manually","description":"Manually return messages of session"}],"default":"notSupported","description":"If loading messages of a previous session should be enabled"},{"displayName":"Response Mode","name":"responseMode","type":"options","options":[{"name":"When Last Node Finishes","value":"lastNode","description":"Returns data of the last-executed node"},{"name":"Using 'Respond to Webhook' Node","value":"responseNode","description":"Response defined in that node"}],"default":"lastNode","description":"When and how to respond to the webhook"},{"displayName":"Require Button Click to Start Chat","name":"showWelcomeScreen","type":"boolean","displayOptions":{"show":{"/mode":["hostedChat"]}},"default":false,"description":"Whether to show the welcome screen at the start of the chat"},{"displayName":"Start Conversation Button Text","name":"getStarted","type":"string","displayOptions":{"show":{"showWelcomeScreen":[true],"/mode":["hostedChat"]}},"default":"New Conversation","placeholder":"e.g. New Conversation","description":"Shown as part of the welcome screen, in the middle of the chat window"},{"displayName":"Subtitle","name":"subtitle","type":"string","displayOptions":{"show":{"/mode":["hostedChat"]}},"default":"Start a chat. We're here to help you 24/7.","placeholder":"e.g. We're here for you","description":"Shown at the top of the chat, under the title"},{"displayName":"Title","name":"title","type":"string","displayOptions":{"show":{"/mode":["hostedChat"]}},"default":"Hi there! ","placeholder":"e.g. Welcome","description":"Shown at the top of the chat"}]}]},
{"displayName":"In-Memory Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemory","description":"Work with your data in In-Memory Vector Store","icon":"fa:database","group":["transform"],"version":1,"defaults":{"name":"In-Memory Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/"}]}},"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"},"displayOptions":{"show":{"mode":["retrieve"]}}},{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."},{"displayName":"The embbded data are stored in the server memory, so they will be lost when the server is restarted. Additionally, if the amount of data is too large, it may cause the server to crash due to insufficient memory.","name":"notice","type":"notice","default":"","displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Clear Store","name":"clearStore","type":"boolean","default":false,"description":"Whether to clear the store before inserting new data","displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}}]},
{"displayName":"In Memory Vector Store Insert","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemoryInsert","icon":"fa:database","group":["transform"],"version":1,"hidden":true,"description":"Insert data into an in-memory vector store","defaults":{"name":"In Memory Vector Store Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/"}]}},"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"The embbded data are stored in the server memory, so they will be lost when the server is restarted. Additionally, if the amount of data is too large, it may cause the server to crash due to insufficient memory.","name":"notice","type":"notice","default":""},{"displayName":"Clear Store","name":"clearStore","type":"boolean","default":false,"description":"Whether to clear the store before inserting new data"},{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."}]},
{"displayName":"In Memory Vector Store Load","name":"@n8n/n8n-nodes-langchain.vectorStoreInMemoryLoad","icon":"fa:database","group":["transform"],"version":1,"hidden":true,"description":"Load embedded data from an in-memory vector store","defaults":{"name":"In Memory Vector Store Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/"}]}},"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Memory Key","name":"memoryKey","type":"string","default":"vector_store_key","description":"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions."},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}]},
{"displayName":"Pinecone Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStorePinecone","description":"Work with your data in Pinecone Vector Store","group":["transform"],"version":1,"defaults":{"name":"Pinecone Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"},"displayOptions":{"show":{"mode":["retrieve"]}}},{"displayName":"Pinecone Index","name":"pineconeIndex","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"pineconeIndexSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Clear Namespace","name":"clearNamespace","type":"boolean","default":false,"description":"Whether to clear the namespace before inserting new data"},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","description":"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.","default":""},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePinecone/pinecone.svg"},
{"displayName":"Pinecone: Insert","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStorePineconeInsert","group":["transform"],"version":1,"description":"Insert data into Pinecone Vector Store index","defaults":{"name":"Pinecone: Insert","color":"#1321A7"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Pinecone Index","name":"pineconeIndex","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"pineconeIndexSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","default":""},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""},{"displayName":"Clear Namespace","name":"clearNamespace","type":"boolean","default":false,"description":"Whether to clear the namespace before inserting new data"}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePineconeInsert/pinecone.svg"},
{"displayName":"Pinecone: Load","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStorePineconeLoad","group":["transform"],"version":1,"description":"Load data from Pinecone Vector Store index","defaults":{"name":"Pinecone: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorepinecone/"}]}},"credentials":[{"name":"pineconeApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Pinecone Index","name":"pineconeIndex","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"pineconeIndexSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Pinecone Namespace","name":"pineconeNamespace","type":"string","default":""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStorePineconeLoad/pinecone.svg"},
{"displayName":"Qdrant Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreQdrant","description":"Work with your data in a Qdrant collection","group":["transform"],"version":1,"defaults":{"name":"Qdrant Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/"}]}},"credentials":[{"name":"qdrantApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"},"displayOptions":{"show":{"mode":["retrieve"]}}},{"displayName":"Qdrant Collection","name":"qdrantCollection","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"qdrantCollectionsSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Collection Config","name":"collectionConfig","type":"json","default":"","description":"JSON options for creating a collection. <a href=\"https://qdrant.tech/documentation/concepts/collections\">Learn more</a>."}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreQdrant/qdrant.svg"},
{"displayName":"Supabase Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreSupabase","description":"Work with your data in Supabase Vector Store","group":["transform"],"version":1,"defaults":{"name":"Supabase Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabase/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"},"displayOptions":{"show":{"mode":["retrieve"]}}},{"displayName":"Table Name","name":"tableName","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"supabaseTableNameSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","description":"Name of the query to use for matching documents"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabase/supabase.svg"},
{"displayName":"Supabase: Insert","hidden":true,"name":"@n8n/n8n-nodes-langchain.vectorStoreSupabaseInsert","group":["transform"],"version":1,"description":"Insert data into Supabase Vector Store index [https://supabase.com/docs/guides/ai/langchain]","defaults":{"name":"Supabase: Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabase/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Please refer to the <a href=\"https://supabase.com/docs/guides/ai/langchain\" target=\"_blank\">Supabase documentation</a> for more information on how to setup your database as a Vector Store.","name":"setupNotice","type":"notice","default":""},{"displayName":"Table Name","name":"tableName","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"supabaseTableNameSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","required":true,"description":"Name of the query to use for matching documents"},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabaseInsert/supabase.svg"},
{"displayName":"Supabase: Load","name":"@n8n/n8n-nodes-langchain.vectorStoreSupabaseLoad","hidden":true,"group":["transform"],"version":1,"description":"Load data from Supabase Vector Store index","defaults":{"name":"Supabase: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoresupabase/"}]}},"credentials":[{"name":"supabaseApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Table Name","name":"tableName","type":"resourceLocator","default":{"mode":"list","value":""},"required":true,"modes":[{"displayName":"From List","name":"list","type":"list","typeOptions":{"searchListMethod":"supabaseTableNameSearch"}},{"displayName":"ID","name":"id","type":"string"}]},{"displayName":"Query Name","name":"queryName","type":"string","default":"match_documents","required":true,"description":"Name of the query to use for matching documents"},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreSupabaseLoad/supabase.svg"},
{"displayName":"Zep Vector Store","name":"@n8n/n8n-nodes-langchain.vectorStoreZep","description":"Work with your data in Zep Vector Store","group":["transform"],"version":1,"defaults":{"name":"Zep Vector Store"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezep/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode;\n\t\t\t\tconst inputs = [{ displayName: \"Embedding\", type: \"ai_embedding\", required: true, maxConnections: 1}]\n\n\t\t\t\tif (['insert', 'load'].includes(mode)) {\n\t\t\t\t\tinputs.push({ displayName: \"\", type: \"main\"})\n\t\t\t\t}\n\n\t\t\t\tif (mode === 'insert') {\n\t\t\t\t\tinputs.push({ displayName: \"Document\", type: \"ai_document\", required: true, maxConnections: 1})\n\t\t\t\t}\n\t\t\t\treturn inputs\n\t\t\t})($parameter)\n\t\t}}","outputs":"={{\n\t\t\t((parameters) => {\n\t\t\t\tconst mode = parameters?.mode ?? 'retrieve';\n\t\t\t\tif (mode === 'retrieve') {\n\t\t\t\t\treturn [{ displayName: \"Vector Store\", type: \"ai_vectorStore\"}]\n\t\t\t\t}\n\t\t\t\treturn [{ displayName: \"\", type: \"main\"}]\n\t\t\t})($parameter)\n\t\t}}","properties":[{"displayName":"Operation Mode","name":"mode","type":"options","noDataExpression":true,"default":"retrieve","options":[{"name":"Get Many","value":"load","description":"Get many ranked documents from vector store for query","action":"Get many ranked documents from vector store for query"},{"name":"Insert Documents","value":"insert","description":"Insert documents into vector store","action":"Insert documents into vector store"},{"name":"Retrieve Documents (For Agent/Chain)","value":"retrieve","description":"Retrieve documents from vector store to be used with AI nodes","action":"Retrieve documents from vector store to be used with AI nodes"}]},{"displayName":"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>","name":"notice","type":"notice","default":"","typeOptions":{"containerClass":"ndv-connection-hint-notice"},"displayOptions":{"show":{"mode":["retrieve"]}}},{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Is Auto Embedded","name":"isAutoEmbedded","type":"boolean","default":true,"description":"Whether to automatically embed documents when they are added"}],"displayOptions":{"show":{"mode":["insert"]}}},{"displayName":"Prompt","name":"prompt","type":"string","default":"","required":true,"description":"Search prompt to retrieve matching documents from the vector store using similarity-based ranking","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Limit","name":"topK","type":"number","default":4,"description":"Number of top results to fetch from vector store","displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["load"]}}},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}],"displayOptions":{"show":{"mode":["retrieve"]}}}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZep/zep.png"},
{"displayName":"Zep Vector Store: Insert","name":"@n8n/n8n-nodes-langchain.vectorStoreZepInsert","hidden":true,"group":["transform"],"version":1,"description":"Insert data into Zep Vector Store index","defaults":{"name":"Zep: Insert"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezep/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":["main",{"displayName":"Document","maxConnections":1,"type":"ai_document","required":true},{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["main"],"properties":[{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Specify the document to load in the document loader sub-node","name":"notice","type":"notice","default":""},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Is Auto Embedded","name":"isAutoEmbedded","type":"boolean","default":true,"description":"Whether to automatically embed documents when they are added"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZepInsert/zep.png"},
{"displayName":"Zep Vector Store: Load","name":"@n8n/n8n-nodes-langchain.vectorStoreZepLoad","hidden":true,"group":["transform"],"version":1,"description":"Load data from Zep Vector Store index","defaults":{"name":"Zep: Load"},"codex":{"categories":["AI"],"subcategories":{"AI":["Vector Stores"]},"resources":{"primaryDocumentation":[{"url":"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstorezep/"}]}},"credentials":[{"name":"zepApi","required":true}],"inputs":[{"displayName":"Embedding","maxConnections":1,"type":"ai_embedding","required":true}],"outputs":["ai_vectorStore"],"outputNames":["Vector Store"],"properties":[{"displayName":"Collection Name","name":"collectionName","type":"string","default":"","required":true},{"displayName":"Options","name":"options","type":"collection","placeholder":"Add Option","default":{},"options":[{"displayName":"Embedding Dimensions","name":"embeddingDimensions","type":"number","default":1536,"description":"Whether to allow using characters from the Unicode surrogate blocks"},{"displayName":"Metadata Filter","name":"metadata","type":"fixedCollection","description":"Metadata to filter the document by","typeOptions":{"multipleValues":true},"default":{},"placeholder":"Add filter field","options":[{"name":"metadataValues","displayName":"Fields to Set","values":[{"displayName":"Name","name":"name","type":"string","default":"","required":true},{"displayName":"Value","name":"value","type":"string","default":""}]}]}]},{"displayName":"Request Options","name":"requestOptions","type":"collection","isNodeSetting":true,"placeholder":"Add Option","default":{},"options":[{"displayName":"Batching","name":"batching","placeholder":"Add Batching","type":"fixedCollection","typeOptions":{"multipleValues":false},"default":{"batch":{}},"options":[{"displayName":"Batching","name":"batch","values":[{"displayName":"Items per Batch","name":"batchSize","type":"number","typeOptions":{"minValue":-1},"default":50,"description":"Input will be split in batches to throttle requests. -1 for disabled. 0 will be treated as 1."},{"displayName":"Batch Interval (ms)","name":"batchInterval","type":"number","typeOptions":{"minValue":0},"default":1000,"description":"Time (in milliseconds) between each batch of requests. 0 for disabled."}]}]},{"displayName":"Ignore SSL Issues","name":"allowUnauthorizedCerts","type":"boolean","noDataExpression":true,"default":false,"description":"Whether to accept the response even if SSL certificate validation is not possible"},{"displayName":"Proxy","name":"proxy","type":"string","default":"","placeholder":"e.g. http://myproxy:3128","description":"HTTP proxy to use. If authentication is required it can be defined as follow: http://username:password@myproxy:3128"},{"displayName":"Timeout","name":"timeout","type":"number","typeOptions":{"minValue":1},"default":10000,"description":"Time in ms to wait for the server to send response headers (and start the response body) before aborting the request"}]}],"iconUrl":"icons/@n8n/n8n-nodes-langchain/dist/nodes/vector_store/VectorStoreZepLoad/zep.png"}
]